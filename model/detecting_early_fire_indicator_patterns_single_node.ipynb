{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33544866",
   "metadata": {},
   "source": [
    "# Detecting Early Fire Indicator Patterns in Multivariate-Time Series Based on a Multi-Sensor Node Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73ba11",
   "metadata": {},
   "source": [
    "## Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2514a4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "# Standard libaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()   \n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "# skLearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "#Sktime\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sktime.datatypes import check_raise\n",
    "from sktime.datatypes import mtype\n",
    "from sktime.datatypes import check_is_mtype\n",
    "from sktime.transformations.panel.padder import PaddingTransformer\n",
    "from sktime.transformations.series.summarize import SummaryTransformer\n",
    "from sktime.datatypes import convert_to\n",
    "from sktime.datatypes import convert\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "\n",
    "# Additional\n",
    "import matplotlib.dates as mdates\n",
    "import time # to claculate the runtime of models\n",
    "from pathlib import Path \n",
    "\n",
    "# SHAP Explanation\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a6218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "data_path = os.path.join(current_dir, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c97ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_export = os.path.join(current_dir, 'export/single_node')\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_export):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(directory_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7e8f5",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aee94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features\n",
    "FEATURES_STATIC = ['CO_Room',\n",
    "                   'H2_Room',\n",
    "                   'VOC_Room_RAW',\n",
    "                   'PM05_Room',\n",
    "                   'PM10_Room',\n",
    "                   ]\n",
    "\n",
    "# Combine to one FEATURES list\n",
    "FEATURES = FEATURES_STATIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05202a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_nodes_variable = False #True #False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18a4ee",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6fee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "# Construct the absolute path to the data file\n",
    "file_name = 'indoor_fire_detection_multisensornodes_dataset_preprocessed.csv'\n",
    "data_file_path = os.path.join(data_path, file_name)\n",
    "\n",
    "X_t = pd.read_csv(data_file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb377161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to int64 data type\n",
    "X_t['date'] = X_t['date'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca0511e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only number of sensornode instead of string name\n",
    "X_t['Sensor_ID'] = X_t['Sensor_ID'].str[-2:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a9b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non relevant labels\n",
    "X_t = X_t.drop(columns = ['scenario_label', \n",
    "                          'progress_label', \n",
    "                          'anomaly_label', \n",
    "                          'ternary_label', \n",
    "#                           'Motion_Room', \n",
    "#                           'Motion_Room_Trend'\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb667197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "X_t = X_t.rename(columns={\"date\": \"timepoints\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927e8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_t in mtype=\"pd-multiindex\" format for sktime\n",
    "# doc.: https://github.com/sktime/sktime/blob/main/examples/AA_datatypes_and_datasets.ipynb\n",
    "\n",
    "X_t.set_index(['interval_label','Sensor_ID', 'timepoints'], inplace=True)\n",
    "\n",
    "X_t = X_t.groupby(level=[0,1,2], sort=True).sum()\n",
    "\n",
    "X_t['timepoints'] = X_t.groupby(['interval_label','Sensor_ID']).cumcount()\n",
    "X_t['timepoints'] = X_t['timepoints'].apply(lambda x: x*10)\n",
    "\n",
    "X_t = X_t.set_index('timepoints', append=True)\n",
    "X_t = X_t.droplevel(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07327cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of sensor IDs\n",
    "list_sensornodes = [8,\n",
    "                    9,\n",
    "                    10,\n",
    "                    11,\n",
    "                    12,\n",
    "                    13,\n",
    "                    14,\n",
    "                    15,\n",
    "                    16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11daa07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoding and decoding of Label column\n",
    "# Encode string labels to integers\n",
    "label_mapping = {\"NoFire\": 0, \"Fire\": 1} # do the label encoding by yourself\n",
    "\n",
    "# Define a function to map labels using the dictionary\n",
    "def map_labels(label):\n",
    "    return label_mapping[label]\n",
    "\n",
    "# Inverse mapping dictionary\n",
    "inverse_label_mapping = {0: \"NoFire\", 1: \"Fire\"}\n",
    "\n",
    "# Use numpy.vectorize to apply the mapping function element-wise\n",
    "X_t['fire_label'] = np.vectorize(map_labels)(X_t['fire_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91294689",
   "metadata": {},
   "source": [
    "## Define model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7221e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Explanation Function\n",
    "def mvts_shap(X_train, X_test, y_train, y_test, X_reference_negative, X_reference_positive):\n",
    "    \n",
    "    i, j, k = X_train.shape # i = intervals, j = features, k = timepoints per interval\n",
    "    u, v, w = X_test.shape\n",
    "    l, s, t = X_reference_negative.shape \n",
    "    \n",
    "    X_train_flat = X_train.reshape(i, j*k)\n",
    "    X_reference_negative_flat = X_reference_negative.reshape(l, s*t) # reshape selected background intervals\n",
    "    X_reference_positive_flat = X_reference_positive.reshape(l, s*t)\n",
    "    X_reference_positive_and_negative_flat = np.concatenate((X_reference_negative_flat, X_reference_positive_flat), axis=0)\n",
    "    \n",
    "    def reshaper(inner_tensor):\n",
    "        return inner_tensor.reshape(inner_tensor.shape[0], j, k)\n",
    "\n",
    "    def inv_reshaper(inner_tensor):\n",
    "        return inner_tensor.reshape(inner_tensor.shape[0], j*k)\n",
    "\n",
    "    param = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'tree_method': 'hist',\n",
    "            'eval_metric': 'logloss',\n",
    "            'seed': 888,\n",
    "            'n_estimators': 500 #500\n",
    "           }\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('reshaper_t', FunctionTransformer(reshaper, inverse_func=inv_reshaper)),\n",
    "        ('tabulariser', MiniRocketMultivariate(num_kernels=10000, n_jobs= -1, random_state=1837)), \n",
    "        ('scaler', StandardScaler()), #'scaler', StandardScaler() # 'scaler', MinMaxScaler()\n",
    "        ('bst', xgb.XGBClassifier(**param))\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train_flat, y_train)\n",
    "    \n",
    "    # Take n random samples from referenz data\n",
    "    masker = shap.maskers.Independent(X_reference_positive_and_negative_flat, 7) # number defines the amount of intervals to be considered\n",
    "    \n",
    "    # fixed background dataset with the same shape as X_train_flat_summary\n",
    "    fixed_background_data = np.full_like(masker.data, 1000)\n",
    "    \n",
    "    # Define Explainer\n",
    "    explainer = shap.KernelExplainer(pipe.predict_proba, masker.data)\n",
    "\n",
    "    # Explanation on 0 to n = sample_size intervals\n",
    "    sample_size = u\n",
    "    sample_idx = np.arange(sample_size)\n",
    "    \n",
    "    # Derive SHAP explanations Explainer\n",
    "    shap_output = explainer.shap_values(X_test[sample_idx, :, :].reshape(sample_size, v * w))\n",
    "    shap_tensor_no_fire = shap_output[0].reshape(sample_size, v, w) \n",
    "    shap_tensor_fire = shap_output[1].reshape(sample_size, v, w)\n",
    "    base_value_shap = explainer.expected_value[1]\n",
    "    \n",
    "    # Get the model's predictions on the test data\n",
    "    model_predictions_encoded = pipe.predict(X_test.reshape(u, v*w))\n",
    "    \n",
    "    # Get predict_proba on test data\n",
    "    model_predict_proba = pipe.predict_proba(X_test.reshape(u, v*w))\n",
    "    \n",
    "    return shap_tensor_no_fire, shap_tensor_fire, model_predictions_encoded, model_predict_proba, base_value_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0d7a75",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c93eedc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f68db16285f4740996ed57c166cfd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensornode: 8\n",
      "Scaler: NoScaling\n",
      "max Shap value NoFire:  0.12521298208273896\n",
      "min Shap value NoFire -0.12576973627545077\n",
      "max Shap value Fire:  0.12576973648691875\n",
      "min Shap value Fire -0.12521298207487594\n",
      "Base SHAP Value (expected_value): 0.5693421346555363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3340edd88e74a0f9b15d534b2224ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 206\u001b[0m\n\u001b[0;32m    203\u001b[0m     X_reference_positive_3d \u001b[38;5;241m=\u001b[39m convert_to(X_background_shap_positive, to_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy3D\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;66;03m# apply mvts_shap function to data #, explanation\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m     shap_values_no_fire, shap_values_fire, y_pred_encoded, pred_proba, base_value_shap \u001b[38;5;241m=\u001b[39m \u001b[43mmvts_shap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m                                                                                                       \u001b[49m\u001b[43mX_test_3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m                                                                                                       \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m                                                                                                       \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m                                                                                                       \u001b[49m\u001b[43mX_reference_negative_3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m                                                                                                       \u001b[49m\u001b[43mX_reference_positive_3d\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m                                                                                                      \u001b[49m\u001b[43m)\u001b[49m                                      \n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# Direct Print outs\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSensornode:\u001b[39m\u001b[38;5;124m'\u001b[39m, sensornode)\n",
      "Cell \u001b[1;32mIn[14], line 50\u001b[0m, in \u001b[0;36mmvts_shap\u001b[1;34m(X_train, X_test, y_train, y_test, X_reference_negative, X_reference_positive)\u001b[0m\n\u001b[0;32m     47\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(sample_size)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Derive SHAP explanations Explainer\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m shap_output \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m shap_tensor_no_fire \u001b[38;5;241m=\u001b[39m shap_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(sample_size, v, w) \u001b[38;5;66;03m#ACHTUNG: [0] muss encoded dem gewünschten Label entsprechen!\u001b[39;00m\n\u001b[0;32m     52\u001b[0m shap_tensor_fire \u001b[38;5;241m=\u001b[39m shap_output[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(sample_size, v, w)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\shap\\explainers\\_kernel.py:242\u001b[0m, in \u001b[0;36mKernel.shap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[0;32m    241\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[1;32m--> 242\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    244\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\shap\\explainers\\_kernel.py:436\u001b[0m, in \u001b[0;36mKernel.explain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight_left \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[0;32m    439\u001b[0m phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\shap\\explainers\\_kernel.py:575\u001b[0m, in \u001b[0;36mKernel.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered:\n\u001b[0;32m    574\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[1;32m--> 575\u001b[0m modelOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modelOut, (pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries)):\n\u001b[0;32m    577\u001b[0m     modelOut \u001b[38;5;241m=\u001b[39m modelOut\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\sklearn\\pipeline.py:546\u001b[0m, in \u001b[0;36mPipeline.predict_proba\u001b[1;34m(self, X, **predict_proba_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 546\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict_proba(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_proba_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\sktime\\transformations\\base.py:545\u001b[0m, in \u001b[0;36mBaseTransformer.transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;66;03m# convert to output mtype\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_conv \u001b[38;5;129;01mand\u001b[39;00m output_conv:\n\u001b[1;32m--> 545\u001b[0m     X_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    547\u001b[0m     X_out \u001b[38;5;241m=\u001b[39m Xt\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\sktime\\transformations\\base.py:1168\u001b[0m, in \u001b[0;36mBaseTransformer._convert_output\u001b[1;34m(self, X, metadata, inverse)\u001b[0m\n\u001b[0;32m   1165\u001b[0m             Xt\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m Xt\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mdroplevel(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1166\u001b[0m         \u001b[38;5;66;03m# else this is only zeros and should be reset to RangeIndex\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1168\u001b[0m             Xt \u001b[38;5;241m=\u001b[39m \u001b[43mXt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1169\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m convert_to(\n\u001b[0;32m   1170\u001b[0m         Xt,\n\u001b[0;32m   1171\u001b[0m         to_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpd_DataFrame_Table\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1172\u001b[0m         as_scitype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1173\u001b[0m         \u001b[38;5;66;03m# no converter store since this is not a \"1:1 back-conversion\"\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;66;03m# else output_scitype is \"Panel\" and no need for conversion\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\pandas\\core\\frame.py:6154\u001b[0m, in \u001b[0;36mDataFrame.reset_index\u001b[1;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[0;32m   6152\u001b[0m     new_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   6153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 6154\u001b[0m     new_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   6155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_duplicates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   6156\u001b[0m     allow_duplicates \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(allow_duplicates, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_duplicates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\pandas\\core\\generic.py:6452\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6342\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   6344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6345\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6346\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6450\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   6451\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6452\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\pandas\\core\\internals\\managers.py:653\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    651\u001b[0m     new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m--> 653\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    654\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:540\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    538\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 540\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create empty list for storing shap values\n",
    "dfs_max_shap_values_fire = []\n",
    "\n",
    "# create empty list for storing plot dfs\n",
    "dfs_plot_sum_shap_values_fire_max_abs_sign = []\n",
    "dfs_plot_sum_shap_values_fire_max_abs_sign_only_fire = []\n",
    "dfs_plot_sum_shap_values_fire_max_abs_only_fire = []\n",
    "dfs_plot_sum_shap_values_max_abs_mean = []\n",
    "dfs_plot_sum_shap_values_max_abs_mean_class_based = []\n",
    "\n",
    "# Define Scaler\n",
    "dict_scaler = {'NoScaling': None,\n",
    "#                'QuantileTransformer10': QuantileTransformer(n_quantiles=10, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer20': QuantileTransformer(n_quantiles=20, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer30': QuantileTransformer(n_quantiles=30, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer5': QuantileTransformer(n_quantiles=5, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer4': QuantileTransformer(n_quantiles=4, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer50': QuantileTransformer(n_quantiles=50, random_state=0, output_distribution='normal'),\n",
    "#                'RobustScaler': RobustScaler(),\n",
    "#                'MaxAbsScaler': MaxAbsScaler(),\n",
    "#                'PowerTransformer': PowerTransformer(),\n",
    "#                'Normalizer': Normalizer(),\n",
    "#                'StandardScaler': StandardScaler(),\n",
    "#                'MinMaxScaler': MinMaxScaler()\n",
    "              }\n",
    "\n",
    "# Decide if diefferent sensornodes should be used for traing and test\n",
    "if multiple_nodes_variable == True:\n",
    "    \n",
    "    for sensornode_train in list_sensornodes:\n",
    "        # Create a list of names for testing excluding the current train_name\n",
    "        test_nodes = [name for name in list_sensornodes if name != sensornode_train]\n",
    "        # Create a new DataFrame with only the locked columns\n",
    "        df_train = X_t.xs(key= sensornode_train, level='Sensor_ID')\n",
    "        # X_train\n",
    "        X_train = df_train[FEATURES]\n",
    "        # y_train\n",
    "        y_train = df_train['fire_label'].groupby('interval_label').first().to_numpy()\n",
    "\n",
    "        for sensornode_test in test_nodes:\n",
    "            # Define df test\n",
    "            df_test = X_t.xs(key= sensornode_test, level='Sensor_ID')\n",
    "            # X_test\n",
    "            X_test = df_test[FEATURES]\n",
    "            # y_test\n",
    "            y_test = df_test['fire_label'].groupby('interval_label').first().to_numpy()\n",
    "\n",
    "            for scaler_name, scaler_instance in dict_scaler.items():\n",
    "\n",
    "                scaler = scaler_instance\n",
    "\n",
    "                # Fit the scaler on X_train\n",
    "                scaler.fit(X_train)\n",
    "\n",
    "                # Transform X_train and X_test separately\n",
    "                X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "                X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "                # generate background referenz data for SHAP explanation\n",
    "                # idea of '.loc[1300:1400]': take only intervals that are background intervals\n",
    "                X_test_background = X_test.loc[1300:1400]  \n",
    "                X_test_background_scaled = X_test_scaled.loc[1300:1400]\n",
    "\n",
    "                # Transform X_train etc. into 3D numpy array\n",
    "                X_train_3d = convert_to(X_train_scaled, to_type=\"numpy3D\") #X_train #X_train_scaled\n",
    "                X_test_3d = convert_to(X_test_scaled, to_type=\"numpy3D\") #X_test #X_test_scaled\n",
    "                X_reference_3d = convert_to(X_test_background_scaled, to_type=\"numpy3D\") #X_test_background #X_test_background_scaled\n",
    "\n",
    "                # apply mvts_shap function to data\n",
    "                shap_values, y_pred, pred_proba = mvts_shap(X_train_3d, X_test_3d, y_train, y_test, X_reference_3d)\n",
    "\n",
    "                # Direct printouts\n",
    "                print('Sensornode_Train:', sensornode_train)\n",
    "                print('Sensornode_Test:', sensornode_test)\n",
    "                print('Scaler:', scaler_name)\n",
    "                print('max Shap value: ', shap_values.max())\n",
    "\n",
    "                # Retransform from 3D array to multiindex df\n",
    "                # convert\n",
    "                df_shap_values = convert(shap_values, from_type=\"numpy3D\", to_type=\"pd-multiindex\")\n",
    "                # Get column names based on original df\n",
    "                columns_names = X_train.columns.tolist()\n",
    "                # Rename the columns\n",
    "                df_shap_values.columns = columns_names\n",
    "                # Multiply the values in the second index level by 10\n",
    "                df_shap_values.index = df_shap_values.index.set_levels(df_shap_values.index.levels[1] * 10, level=1)\n",
    "                # rename index\n",
    "                df_shap_values.index = df_shap_values.index.set_names('interval_label', level=0)\n",
    "\n",
    "\n",
    "                # Add model Predictions to df_test \n",
    "                # Convert the NumPy array to a pandas DataFrame\n",
    "                y_pred_df = pd.DataFrame({'model_prediction': y_pred})\n",
    "\n",
    "                # Add a second column containing the interval_label\n",
    "                y_pred_df['interval_label'] = df_test.index.get_level_values('interval_label').unique()\n",
    "\n",
    "                # Set 'index_column' as the index\n",
    "                y_pred_df.set_index('interval_label', inplace=True)\n",
    "\n",
    "                # Convert the DataFrame to a dictionary\n",
    "                y_pred_dict = y_pred_df.to_dict(orient='dict')['model_prediction']\n",
    "\n",
    "                # add model_prediction to df_test_export\n",
    "                df_test_export = df_test.copy()\n",
    "                df_test_export['model_prediction'] = df_test_export.index.get_level_values('interval_label').map(y_pred_dict)\n",
    "\n",
    "                # Reset Interval_label index so that it can be merged with df_shap_values\n",
    "                # Calculate the offset to align df_2 with df_1\n",
    "                offset = df_shap_values.index.get_level_values('interval_label').min() - df_test_export.index.get_level_values('interval_label').min()\n",
    "                # Adjust the index of df_2\n",
    "                df_test_export.index = df_test_export.index.set_levels(df_test_export.index.levels[0] + offset, level=0)\n",
    "\n",
    "                # Export df_test\n",
    "                # Construct the absolute path to the data file\n",
    "                file_name = f'df_test_elba_rocket_train{sensornode_train}_test{sensornode_test}_{scaler_name}.csv'\n",
    "                data_file_path = os.path.join(directory_export, file_name)\n",
    "                df_test_export.to_csv(data_file_path)\n",
    "\n",
    "                # Export df_shap_values\n",
    "                file_name = f'df_shap_values_elba_rocket_train{sensornode_train}_test{sensornode_test}_{scaler_name}.csv' \n",
    "                data_file_path = os.path.join(directory_export, file_name)\n",
    "                df_shap_values.to_csv(data_file_path)\n",
    "                \n",
    "    \n",
    "else:\n",
    "    if multiple_nodes_variable == False:\n",
    "        for scaler_name, scaler_instance in dict_scaler.items(): \n",
    "        \n",
    "            # Build model for every unique sensor node \n",
    "            for sensornode in list_sensornodes:\n",
    "\n",
    "                # Create a new DataFrame with only the locked columns\n",
    "                data_temp = X_t.xs(key= sensornode, level='Sensor_ID')\n",
    "\n",
    "                # Derive Train and hold-out (test) set\n",
    "                split_interval = 2000 #1300\n",
    "                df_train = data_temp.loc[data_temp.index.get_level_values('interval_label') <= split_interval]\n",
    "                df_test = data_temp.loc[data_temp.index.get_level_values('interval_label') > split_interval]\n",
    "\n",
    "                # X_train\n",
    "                X_train = df_train[FEATURES]\n",
    "                # X_test\n",
    "                X_test = df_test[FEATURES]\n",
    "                # y_train\n",
    "                y_train = df_train.groupby('interval_label')['fire_label'].first().to_numpy()\n",
    "                # y_test\n",
    "                y_test = df_test.groupby('interval_label')['fire_label'].first().to_numpy()\n",
    "\n",
    "                # generate background referenz data for SHAP explanation\n",
    "                # Define number of Intervals\n",
    "                sample_size = 5\n",
    "\n",
    "                # Randomly select n rows where \"NoFire\" occurs in the \"fire_label\" column based on 'Interval_label'\n",
    "                # Find all intervals where 'fire_label' is 'NoFire' for each group \n",
    "                selected_intervals_negative = list(df_train.loc[df_train['fire_label'] == 0].groupby(level='interval_label').groups.keys())\n",
    "                selected_intervals_positive = list(df_train.loc[df_train['fire_label'] == 1].groupby(level='interval_label').groups.keys())\n",
    "                # sample n intervals\n",
    "                selected_intervals_negative = np.random.choice(selected_intervals_negative, size=min(sample_size, len(selected_intervals_negative)), replace=False)\n",
    "                selected_intervals_positive = np.random.choice(selected_intervals_positive, size=min(sample_size, len(selected_intervals_positive)), replace=False)\n",
    "\n",
    "\n",
    "                if scaler_instance is not None:\n",
    "                    # Define scaler\n",
    "                    scaler = scaler_instance\n",
    "\n",
    "                    # Fit the scaler on X_train\n",
    "                    scaler.fit(X_train)\n",
    "\n",
    "                    # Transform X_train and X_test separately\n",
    "                    X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "                    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "                    # Use pd.IndexSlice to select rows based on the array of indexes\n",
    "                    X_background_shap_negative_scaled = X_train_scaled.loc[pd.IndexSlice[selected_intervals_negative, :], :]\n",
    "                    X_background_shap_positive_scaled = X_train_scaled.loc[pd.IndexSlice[selected_intervals_positive, :], :]\n",
    "\n",
    "                    # Transform X_train etc. into 3D numpy array\n",
    "                    X_train_3d_scaled = convert_to(X_train_scaled, to_type=\"numpy3D\")\n",
    "                    X_test_3d_scaled = convert_to(X_test_scaled, to_type=\"numpy3D\")\n",
    "                    X_reference_negative_3d_scaled = convert_to(X_background_shap_negative_scaled, to_type=\"numpy3D\")\n",
    "                    X_reference_positive_3d_scaled = convert_to(X_background_shap_positive_scaled, to_type=\"numpy3D\")                \n",
    "\n",
    "                    # apply mvts_shap function to data #, explanation\n",
    "                    shap_values_no_fire, shap_values_fire, y_pred_encoded, pred_proba, base_value_shap = mvts_shap(X_train_3d_scaled,\n",
    "                                                                                                                   X_test_3d_scaled,\n",
    "                                                                                                                   y_train,\n",
    "                                                                                                                   y_test,\n",
    "                                                                                                                   X_reference_negative_3d_scaled,\n",
    "                                                                                                                   X_reference_positive_3d_scaled\n",
    "                                                                                                                  )\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # Use pd.IndexSlice to select rows based on the array of indexes\n",
    "                    X_background_shap_negative = X_train.loc[pd.IndexSlice[selected_intervals_negative, :], :]\n",
    "                    X_background_shap_positive = X_train.loc[pd.IndexSlice[selected_intervals_positive, :], :]\n",
    "\n",
    "                    # Transform X_train etc. into 3D numpy array (unscaled)\n",
    "                    X_train_3d = convert_to(X_train, to_type=\"numpy3D\")\n",
    "                    X_test_3d = convert_to(X_test, to_type=\"numpy3D\")\n",
    "                    X_reference_negative_3d = convert_to(X_background_shap_negative, to_type=\"numpy3D\")\n",
    "                    X_reference_positive_3d = convert_to(X_background_shap_positive, to_type=\"numpy3D\")\n",
    "\n",
    "                    # apply mvts_shap function to data #, explanation\n",
    "                    shap_values_no_fire, shap_values_fire, y_pred_encoded, pred_proba, base_value_shap = mvts_shap(X_train_3d,\n",
    "                                                                                                                       X_test_3d,\n",
    "                                                                                                                       y_train,\n",
    "                                                                                                                       y_test,\n",
    "                                                                                                                       X_reference_negative_3d,\n",
    "                                                                                                                       X_reference_positive_3d\n",
    "                                                                                                                      )                                      \n",
    "\n",
    "                # Direct Print outs\n",
    "                print('Sensornode:', sensornode)\n",
    "                print('Scaler:', scaler_name)\n",
    "                # print(explanation)\n",
    "                print('max Shap value NoFire: ', shap_values_no_fire.max())\n",
    "                print('min Shap value NoFire', shap_values_no_fire.min())\n",
    "                print('max Shap value Fire: ', shap_values_fire.max())\n",
    "                print('min Shap value Fire', shap_values_fire.min())\n",
    "                print('Base SHAP Value (expected_value):', base_value_shap)\n",
    "\n",
    "                # Inverse transform the encoded labels to decode them\n",
    "                y_pred = np.vectorize(inverse_label_mapping.get)(y_pred_encoded)\n",
    "\n",
    "                # Retransform from 3D array to multiindex df\n",
    "                # convert\n",
    "                df_shap_values_no_fire = convert(shap_values_no_fire, from_type=\"numpy3D\", to_type=\"pd-multiindex\")\n",
    "                df_shap_values_fire = convert(shap_values_fire, from_type=\"numpy3D\", to_type=\"pd-multiindex\")             \n",
    "                # Get column names based on original df\n",
    "                columns_names = X_train.columns.tolist()\n",
    "                # Rename the columns\n",
    "                df_shap_values_no_fire.columns = columns_names\n",
    "                df_shap_values_fire.columns = columns_names\n",
    "                # Multiply the values in the second index level by 10\n",
    "                df_shap_values_no_fire.index = df_shap_values_no_fire.index.set_levels(df_shap_values_no_fire.index.levels[1] * 10, level=1)\n",
    "                df_shap_values_fire.index = df_shap_values_fire.index.set_levels(df_shap_values_fire.index.levels[1] * 10, level=1)\n",
    "                # Rename index\n",
    "                df_shap_values_no_fire.index = df_shap_values_no_fire.index.set_names('interval_label', level=0)\n",
    "                df_shap_values_fire.index = df_shap_values_fire.index.set_names('interval_label', level=0)\n",
    "\n",
    "                # Safe max_shap_values per feature and sensor node for fire class\n",
    "                df_max_shap_values_fire_temp = pd.DataFrame(df_shap_values_fire.max()).reset_index().rename(columns={0: str(sensornode), 'index': 'feature'})\n",
    "                dfs_max_shap_values_fire.append(df_max_shap_values_fire_temp)\n",
    "\n",
    "                # Add model predictions to df_test \n",
    "                # Convert the NumPy array to a pandas DataFrame\n",
    "                y_pred_df = pd.DataFrame({'model_prediction': y_pred})\n",
    "\n",
    "                # Add a second column containing the interval_label\n",
    "                y_pred_df['interval_label'] = df_test.index.get_level_values('interval_label').unique()\n",
    "\n",
    "                # Set 'index_column' as the index\n",
    "                y_pred_df.set_index('interval_label', inplace=True)\n",
    "\n",
    "                # Convert the DataFrame to a dictionary\n",
    "                y_pred_dict = y_pred_df.to_dict(orient='dict')['model_prediction'] \n",
    "\n",
    "                # add model_prediction to df_test_export\n",
    "                df_test_export = df_test.copy()\n",
    "                df_test_export['model_prediction'] = df_test_export.index.get_level_values('interval_label').map(y_pred_dict)\n",
    "\n",
    "                # encode the true label in df_test_export\n",
    "                df_test_export['fire_label'] = np.vectorize(inverse_label_mapping.get)(df_test_export['fire_label'])\n",
    "\n",
    "                # Reset Interval_label index so that it can be merged with df_shap_values\n",
    "                # Calculate the offset to align df_2 with df_1\n",
    "                offset = df_shap_values_no_fire.index.get_level_values('interval_label').min() - df_test_export.index.get_level_values('interval_label').min()\n",
    "                # Adjust the index of df_2\n",
    "                df_test_export.index = df_test_export.index.set_levels(df_test_export.index.levels[0] + offset, level=0)\n",
    "\n",
    "                # Export df_test\n",
    "                # Construct the absolute path to the data file\n",
    "                file_name = f'df_test_elba_rocket_{sensornode}_{scaler_name}.csv'\n",
    "                data_file_path = os.path.join(directory_export, file_name)\n",
    "                df_test_export.to_csv(data_file_path)\n",
    "\n",
    "                # Export df_shap_values\n",
    "                file_name = f'df_shap_values_elba_rocket_nofire_{sensornode}_{scaler_name}.csv'\n",
    "                data_file_path = os.path.join(directory_export, file_name)\n",
    "                df_shap_values_no_fire.to_csv(data_file_path) \n",
    "\n",
    "                # Export df_shap_values\n",
    "                file_name = f'df_shap_values_elba_rocket_fire_{sensornode}_{scaler_name}.csv'\n",
    "                data_file_path = os.path.join(directory_export, file_name)\n",
    "                df_shap_values_fire.to_csv(data_file_path) \n",
    "\n",
    "                # derive max absolute values by respecting the sign (fire and no_fire intervals)\n",
    "                # create new df to derive indexes\n",
    "                df_indexes = df_shap_values_fire.copy()\n",
    "                # find indexes of maximum absolute value \n",
    "                df_indexes = df_indexes.groupby('interval_label').apply(lambda group: group.loc[:, group.columns != 'fire_label'].abs().idxmax())\n",
    "                # replace indexes by shap values\n",
    "                df_shap_values_fire_max_abs_sign = df_indexes.copy()\n",
    "                for col in df_indexes.columns:\n",
    "                    df_shap_values_fire_max_abs_sign[col] = [df_shap_values_fire.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "                # replace indexes by measurement values\n",
    "                df_measurement_values_fire_max_abs_sign = df_indexes.copy()\n",
    "                for col in df_indexes.columns:\n",
    "                    df_measurement_values_fire_max_abs_sign[col] = [df_test_export.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "                # derive scaled measurement values for coloring feature size in plot\n",
    "                columns_to_bin = df_measurement_values_fire_max_abs_sign.columns\n",
    "                df_measurement_values_fire_max_abs_sign_scaled = df_measurement_values_fire_max_abs_sign.copy()\n",
    "                # Define the number of bins (adjust as needed)\n",
    "                num_bins = 5\n",
    "                # Loop through each column and perform binning, scaling, and conversion to integers\n",
    "                for column in columns_to_bin:\n",
    "                    # Create bins using cut\n",
    "                    df_measurement_values_fire_max_abs_sign_scaled[column] = pd.cut(df_measurement_values_fire_max_abs_sign_scaled[column], bins=num_bins, labels=False)\n",
    "                    # Scale the values to the range [0, 10]\n",
    "                    df_measurement_values_fire_max_abs_sign_scaled[column] = df_measurement_values_fire_max_abs_sign_scaled[column] * ((5 - 1) / (num_bins - 1)) + 1\n",
    "                    # Convert scaled values to integers\n",
    "                    df_measurement_values_fire_max_abs_sign_scaled[column] = df_measurement_values_fire_max_abs_sign_scaled[column].astype(int)  \n",
    "                # melt the scaled df in the same form as the shap value for plot\n",
    "                df_measurement_values_fire_max_abs_sign_scaled_melted = pd.melt(df_measurement_values_fire_max_abs_sign_scaled, var_name='feature', value_name='size')\n",
    "                df_measurement_values_fire_max_abs_sign_scaled_melted['sensornode'] = str(sensornode)\n",
    "                df_measurement_values_fire_max_abs_sign_scaled_melted['feature'] = df_measurement_values_fire_max_abs_sign_scaled_melted['feature'].apply(lambda x: x.split('_')[0])\n",
    "                # melt shap values df\n",
    "                df_melted = pd.melt(df_shap_values_fire_max_abs_sign, var_name='feature', value_name='shap_value')\n",
    "                # swarmplot single node\n",
    "                df_plot = df_melted.copy()\n",
    "                df_plot['sensornode'] = str(sensornode)\n",
    "                df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "                # add feature size to df plot\n",
    "                list_feature_sizes = df_measurement_values_fire_max_abs_sign_scaled_melted['size'].to_list()\n",
    "                df_plot['size'] = list_feature_sizes\n",
    "                # append df_plot to list\n",
    "                dfs_plot_sum_shap_values_fire_max_abs_sign.append(df_plot)\n",
    "\n",
    "\n",
    "                # derive max absolute values by respecting the sign (only fire intervals)\n",
    "                # create new df to derive indexes\n",
    "                df_indexes = df_shap_values_fire.copy()\n",
    "                # add fire_label to select only fire intervals\n",
    "                df_indexes['fire_label'] = df_test_export['fire_label'].to_list()[:len(df_shap_values_fire)]\n",
    "                # select only fire intervals\n",
    "                df_indexes = df_indexes.loc[df_indexes.fire_label == 'Fire'] #Fire\n",
    "                # find indexes of maximum absolute value \n",
    "                df_indexes = df_indexes.groupby('interval_label').apply(lambda group: group.loc[:, group.columns != 'fire_label'].abs().idxmax())\n",
    "                # replace indexes by shap values\n",
    "                df_shap_values_fire_max_abs_sign = df_indexes.copy()\n",
    "                for col in df_indexes.columns:\n",
    "                    df_shap_values_fire_max_abs_sign[col] = [df_shap_values_fire.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "                # replace indexes by measurement values\n",
    "                df_measurement_values_fire_max_abs_sign = df_indexes.copy()\n",
    "                for col in df_indexes.columns:\n",
    "                    df_measurement_values_fire_max_abs_sign[col] = [df_test_export.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "                # derive scaled measurement values for coloring feature size in plot\n",
    "                columns_to_bin = df_measurement_values_fire_max_abs_sign.columns\n",
    "                df_measurement_values_fire_max_abs_sign_scaled = df_measurement_values_fire_max_abs_sign.copy()\n",
    "                # Define the number of bins (adjust as needed)\n",
    "                num_bins = 5\n",
    "                # Loop through each column and perform binning, scaling, and conversion to integers\n",
    "                for column in columns_to_bin:\n",
    "                    # Create bins using cut\n",
    "                    df_measurement_values_fire_max_abs_sign_scaled[column] = pd.cut(df_measurement_values_fire_max_abs_sign_scaled[column], bins=num_bins, labels=False)\n",
    "                    # Scale the values to the range [0, 10]\n",
    "                    df_measurement_values_fire_max_abs_sign_scaled[column] = df_measurement_values_fire_max_abs_sign_scaled[column] * ((5 - 1) / (num_bins - 1)) + 1\n",
    "                    # Convert scaled values to integers\n",
    "                    df_measurement_values_fire_max_abs_sign_scaled[column] = df_measurement_values_fire_max_abs_sign_scaled[column].astype(int)  \n",
    "                # melt the scaled df in the same form as the shap value for plot\n",
    "                df_measurement_values_fire_max_abs_sign_scaled_melted = pd.melt(df_measurement_values_fire_max_abs_sign_scaled, var_name='feature', value_name='size')\n",
    "                df_measurement_values_fire_max_abs_sign_scaled_melted['sensornode'] = str(sensornode)\n",
    "                df_measurement_values_fire_max_abs_sign_scaled_melted['feature'] = df_measurement_values_fire_max_abs_sign_scaled_melted['feature'].apply(lambda x: x.split('_')[0])\n",
    "                # melt shap values df\n",
    "                df_melted = pd.melt(df_shap_values_fire_max_abs_sign, var_name='feature', value_name='shap_value')\n",
    "                # swarmplot single node\n",
    "                df_plot = df_melted.copy()\n",
    "                df_plot['sensornode'] = str(sensornode)\n",
    "                df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "                # add feature size to df plot\n",
    "                # df1.merge(df2, how='inner', on='a') # (alternative)\n",
    "                list_feature_sizes = df_measurement_values_fire_max_abs_sign_scaled_melted['size'].to_list()\n",
    "                df_plot['size'] = list_feature_sizes\n",
    "                # append df_plot to list\n",
    "                dfs_plot_sum_shap_values_fire_max_abs_sign_only_fire.append(df_plot)\n",
    "\n",
    "\n",
    "                # summarize timepoints per interval by max absolute shap values ignoring sign (fire and no_fire intervals)\n",
    "                # create new df to derive indexes\n",
    "                df_indexes = df_shap_values_fire.copy()\n",
    "                # find indexes of maximum absolute value \n",
    "                df_indexes = df_indexes.groupby('interval_label').apply(lambda group: group.loc[:, group.columns != 'fire_label'].abs().idxmax())\n",
    "                # replace indexes by shap values\n",
    "                df_shap_values_fire_max_abs = df_indexes.copy()\n",
    "                for col in df_indexes.columns:\n",
    "                    df_shap_values_fire_max_abs[col] = [np.abs(df_shap_values_fire.loc[tuple(val), col]) for val in df_indexes[col]]\n",
    "                # replace indexes by measurement values\n",
    "                df_measurement_values_fire_max_abs = df_indexes.copy()\n",
    "                for col in df_indexes.columns:\n",
    "                    df_measurement_values_fire_max_abs[col] = [df_test_export.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "                # define columns to bin \n",
    "                columns_to_bin = df_measurement_values_fire_max_abs.columns\n",
    "                df_measurement_values_fire_max_abs_scaled = df_measurement_values_fire_max_abs.copy()\n",
    "                # Define the number of bins (adjust as needed)\n",
    "                num_bins = 5\n",
    "                # Loop through each column and perform binning, scaling, and conversion to integers\n",
    "                for column in columns_to_bin:\n",
    "                    # Create bins using cut\n",
    "                    df_measurement_values_fire_max_abs_scaled[column] = pd.cut(df_measurement_values_fire_max_abs_scaled[column], bins=num_bins, labels=False)\n",
    "                    # Scale the values to the range [0, 10]\n",
    "                    df_measurement_values_fire_max_abs_scaled[column] = df_measurement_values_fire_max_abs_scaled[column] * ((5 - 1) / (num_bins - 1)) + 1\n",
    "                    # Convert scaled values to integers\n",
    "                    df_measurement_values_fire_max_abs_scaled[column] = df_measurement_values_fire_max_abs_scaled[column].astype(int)  \n",
    "                # melt the scaled df in the same form as the shap value for plot\n",
    "                df_measurement_values_fire_max_abs_scaled_melted = pd.melt(df_measurement_values_fire_max_abs_scaled, var_name='feature', value_name='size')\n",
    "                df_measurement_values_fire_max_abs_scaled_melted['sensornode'] = str(sensornode)\n",
    "                df_measurement_values_fire_max_abs_scaled_melted['feature'] = df_measurement_values_fire_max_abs_scaled_melted['feature'].apply(lambda x: x.split('_')[0])  \n",
    "                # melt shap values df\n",
    "                df_melted = pd.melt(df_shap_values_fire_max_abs, var_name='feature', value_name='shap_value')\n",
    "                # swarmplot single node\n",
    "                df_plot = df_melted.copy()\n",
    "                df_plot['sensornode'] = str(sensornode)\n",
    "                df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "                # add feature size to df plot\n",
    "                list_feature_sizes = df_measurement_values_fire_max_abs_scaled_melted['size'].to_list()\n",
    "                df_plot['size'] = list_feature_sizes\n",
    "                # append df_plot to list\n",
    "                dfs_plot_sum_shap_values_fire_max_abs_only_fire.append(df_plot)\n",
    "\n",
    "\n",
    "                # summarize timepoints per interval by max absolute mean shap values (barplot)\n",
    "                df_shap_values_fire_max_abs_mean = df_shap_values_fire.copy()\n",
    "                df_shap_values_fire_max_abs_mean = df_shap_values_fire_max_abs_mean.groupby(level='interval_label').max().abs().mean().to_frame()\n",
    "                df_shap_values_fire_max_abs_mean = df_shap_values_fire_max_abs_mean.reset_index().rename(columns = {0: \"shap_value\", 'index':'feature'})\n",
    "                # baplot single node\n",
    "                df_plot = df_shap_values_fire_max_abs_mean.copy()\n",
    "                df_plot['sensornode'] = str(sensornode)\n",
    "                df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "                df_plot = df_plot.sort_values(by=['shap_value', 'feature'], ascending=False)\n",
    "                # append df_plot to list\n",
    "                dfs_plot_sum_shap_values_max_abs_mean.append(df_plot)\n",
    "\n",
    "\n",
    "                # summarize timepoints per interval by max absolute mean shap values class based\n",
    "                df_shap_values_fire_max_abs_mean_classes = df_shap_values_fire.copy()\n",
    "                df_shap_values_fire_max_abs_mean_classes['fire_label'] = df_test_export['fire_label'].to_list()[:len(df_shap_values_fire)]\n",
    "                df_fire_and_interval_labels_temp = df_shap_values_fire_max_abs_mean_classes[['fire_label']].copy()\n",
    "                list_fire_and_interval_labels_temp = df_fire_and_interval_labels_temp.groupby(level = 'interval_label')['fire_label'].first().to_list()\n",
    "                df_shap_values_fire_max_abs_mean_classes = df_shap_values_fire_max_abs_mean_classes.groupby(level = 'interval_label').apply(lambda group: group.loc[:, group.columns != 'fire_label'].max().abs())\n",
    "                # add fire_label\n",
    "                df_shap_values_fire_max_abs_mean_classes['fire_label'] = list_fire_and_interval_labels_temp\n",
    "                # Group by 'fire_label' and calculate the mean for each feature\n",
    "                df_shap_values_fire_max_abs_mean_classes = df_shap_values_fire_max_abs_mean_classes.groupby('fire_label').mean()\n",
    "                # reset index\n",
    "                df_shap_values_fire_max_abs_mean_classes = df_shap_values_fire_max_abs_mean_classes.reset_index(drop=False)\n",
    "                # barplot single node\n",
    "                df_plot = df_shap_values_fire_max_abs_mean_classes.copy()\n",
    "                df_plot = pd.melt(df_plot, id_vars=['fire_label'], var_name='feature', value_name='shap_value')\n",
    "                df_plot['sensornode'] = str(sensornode)\n",
    "                df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "                df_plot = df_plot.sort_values(by=['shap_value', 'feature'], ascending=False)\n",
    "                # append df_plot to list\n",
    "                dfs_plot_sum_shap_values_max_abs_mean_class_based.append(df_plot)\n",
    "\n",
    "\n",
    "            # concat dfs_plot\n",
    "            df_plot_sum_shap_values_fire_max_abs_sign  = pd.concat(dfs_plot_sum_shap_values_fire_max_abs_sign)\n",
    "            df_plot_sum_shap_values_fire_max_abs_sign_only_fire = pd.concat(dfs_plot_sum_shap_values_fire_max_abs_sign_only_fire)\n",
    "            df_plot_sum_shap_values_fire_max_abs_only_fire = pd.concat(dfs_plot_sum_shap_values_fire_max_abs_only_fire)\n",
    "            df_plot_sum_shap_values_max_abs_mean = pd.concat(dfs_plot_sum_shap_values_max_abs_mean)\n",
    "            df_plot_sum_shap_values_max_abs_mean_class_based = pd.concat(dfs_plot_sum_shap_values_max_abs_mean_class_based)\n",
    "\n",
    "\n",
    "            # Define Columns order in facetgrid\n",
    "            col_order = ['8',\n",
    "                         '9', \n",
    "                         '10',\n",
    "                         '11',\n",
    "                         '12',\n",
    "                         '13',\n",
    "                         '14',\n",
    "                         '15',\n",
    "                         '16']\n",
    "\n",
    "            # PLot df_plot_sum_shap_values_fire_max_abs_sign\n",
    "            # Create a FacetGrid\n",
    "            feature_order = ['CO','H2','PM05','PM10','VOC']\n",
    "            g = sns.FacetGrid(data=df_plot_sum_shap_values_fire_max_abs_sign, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, hue = 'size', palette=\"flare\", sharex=False) \n",
    "            # Map a swarmplot to the FacetGrid\n",
    "            g.map(sns.stripplot, \"shap_value\", \"feature\", orient=\"h\", alpha=.8, s=2, order=feature_order) #swarmplot\n",
    "            g.map(sns.violinplot, \"shap_value\", \"feature\", inner=None, color=\"lightgray\", linewidth=0.5, order=feature_order)\n",
    "            # Set axis labels and adjust plot layout\n",
    "            # g.add_legend(title = 'feature value')\n",
    "            # Add legend\n",
    "            g.add_legend(title='feature value')\n",
    "            # axes titles\n",
    "            g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "            # Save the plot to a PNG file\n",
    "            file_name = f'shap_values_max_abs_sign_single_node_model_swarmplot_{scaler_name}.png'\n",
    "            # Create the file path using an f-string\n",
    "            file_path = os.path.join(directory_export, file_name)\n",
    "            plt.savefig(file_path)\n",
    "\n",
    "            # PLot df_plot_sum_shap_values_fire_max_abs_sign_only_fire\n",
    "            # Create a FacetGrid\n",
    "            feature_order = ['CO','H2','PM05','PM10','VOC']\n",
    "            g = sns.FacetGrid(data=df_plot_sum_shap_values_fire_max_abs_sign_only_fire, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, hue = 'size', palette=\"flare\")\n",
    "            # Map a swarmplot to the FacetGrid\n",
    "            g.map(sns.stripplot, \"shap_value\", \"feature\", orient=\"h\", alpha=.8, s=2, order=feature_order) #swarmplot\n",
    "            g.map(sns.violinplot, \"shap_value\", \"feature\", inner=None, color=\"lightgray\", linewidth=0.5, order=feature_order)\n",
    "            # Set axis labels and adjust plot layout\n",
    "            # g.add_legend(title = 'feature value')\n",
    "            # Add legend\n",
    "            g.add_legend(title='feature value')\n",
    "            # axes titles\n",
    "            g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "            # Save the plot to a PNG file\n",
    "            file_name = f'shap_values_max_abs_sign_only_fire_intervals_single_node_model_swarmplot_{scaler_name}.png'\n",
    "            # Create the file path using an f-string\n",
    "            file_path = os.path.join(directory_export, file_name)\n",
    "            plt.savefig(file_path)\n",
    "\n",
    "            # PLot df_plot_sum_shap_values_fire_max_abs_only_fire\n",
    "            # Create a FacetGrid\n",
    "            feature_order = ['CO','H2','PM05','PM10','VOC']\n",
    "            g = sns.FacetGrid(data=df_plot_sum_shap_values_fire_max_abs_only_fire, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, hue = 'size', palette=\"flare\")\n",
    "            # Map a swarmplot to the FacetGrid\n",
    "            g.map(sns.stripplot, \"shap_value\", \"feature\", orient=\"h\", alpha=.8, s=2, order=feature_order) #swarmplot\n",
    "            g.map(sns.violinplot, \"shap_value\", \"feature\", inner=None, color=\"lightgray\", linewidth=0.5, order=feature_order)\n",
    "            # Set axis labels and adjust plot layout\n",
    "            # g.add_legend(title = 'feature value')\n",
    "            # Add legend\n",
    "            g.add_legend(title='feature value')\n",
    "            # axes titles\n",
    "            g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "            # Save the plot to a PNG file\n",
    "            file_name = f'shap_values_max_abs_intervals_single_node_model_swarmplot_{scaler_name}.png'\n",
    "            # Create the file path using an f-string\n",
    "            file_path = os.path.join(directory_export, file_name)\n",
    "            plt.savefig(file_path)\n",
    "\n",
    "            # PLot df_plot_sum_shap_values_max_abs_mean (barplot)\n",
    "            # Create a FacetGrid\n",
    "            g = sns.FacetGrid(data=df_plot_sum_shap_values_max_abs_mean, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, sharey=False)\n",
    "            # Map a swarmplot to the FacetGrid\n",
    "            g.map(sns.barplot, \"shap_value\", \"feature\", orient=\"h\", alpha=.8)\n",
    "            # Set axis labels and adjust plot layout\n",
    "            g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "            plt.tight_layout()\n",
    "            # Save the plot to a PNG file\n",
    "            file_name = f'shap_values_max_abs_mean_single_node_model_barplot_{scaler_name}.png'\n",
    "            # Create the file path using an f-string\n",
    "            file_path = os.path.join(directory_export, file_name)\n",
    "            plt.savefig(file_path)\n",
    "\n",
    "\n",
    "            # PLot df_plot_sum_shap_values_max_abs_mean_class_based (barplot)\n",
    "            # Create a FacetGrid with Seaborn\n",
    "            g = sns.FacetGrid(data = df_plot_sum_shap_values_max_abs_mean_class_based, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, sharey=False)\n",
    "            g.map_dataframe(sns.barplot,  x=\"shap_value\", y=\"feature\", hue=\"fire_label\", orient ='h', palette=\"Set1\", errorbar=None)\n",
    "            g.add_legend()\n",
    "            g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "            # Save the plot to a PNG file\n",
    "            file_name = f'shap_values_max_abs_mean_single_node_model_class_based_barplot_{scaler_name}.png'\n",
    "            # Create the file path using an f-string\n",
    "            file_path = os.path.join(directory_export, file_name)\n",
    "            plt.savefig(file_path)\n",
    "\n",
    "\n",
    "            # Concatenate the list of DataFrames into a single DataFrame\n",
    "            df_max_shap_values_fire_sum = pd.concat(dfs_max_shap_values_fire, axis=1)\n",
    "            df_max_shap_values_fire_sum = df_max_shap_values_fire_sum.loc[:, ~df_max_shap_values_fire_sum.columns.duplicated()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
