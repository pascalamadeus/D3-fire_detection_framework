{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33544866",
   "metadata": {},
   "source": [
    "# Detecting Early Fire Indicator Patterns in Multivariate-Time Series Based on a Multi-Sensor Node Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73ba11",
   "metadata": {},
   "source": [
    "## Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2514a4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "# Standard libaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()   \n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "# skLearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "#Sktime\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sktime.datatypes import check_raise\n",
    "from sktime.datatypes import mtype\n",
    "from sktime.datatypes import check_is_mtype\n",
    "from sktime.transformations.panel.padder import PaddingTransformer\n",
    "from sktime.transformations.series.summarize import SummaryTransformer\n",
    "from sktime.datatypes import convert_to\n",
    "from sktime.datatypes import convert\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "\n",
    "# Additional\n",
    "import matplotlib.dates as mdates\n",
    "import time # to claculate the runtime of models\n",
    "from pathlib import Path \n",
    "\n",
    "# SHAP Explanation\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a6218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "data_path = os.path.join(current_dir, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c97ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_export = os.path.join(current_dir, 'export/network')\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_export):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(directory_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7e8f5",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aee94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features\n",
    "FEATURES_STATIC = ['CO_Room',\n",
    "                    'H2_Room',\n",
    "                    'VOC_Room_RAW',\n",
    "                    'PM05_Room',\n",
    "                    'PM10_Room',\n",
    "                   ]\n",
    "\n",
    "# Combine to one FEATURES list\n",
    "FEATURES = FEATURES_STATIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1fb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sensornodes\n",
    "sensornodes_list = [8,\n",
    "                    9,\n",
    "                    10,\n",
    "                    11,\n",
    "                    12,\n",
    "                    13,\n",
    "                    14,\n",
    "                    15,\n",
    "                    16\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18a4ee",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6fee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "# Construct the absolute path to the data file\n",
    "file_name = 'indoor_fire_detection_multisensornodes_dataset_preprocessed.csv'\n",
    "data_file_path = os.path.join(data_path, file_name)\n",
    "\n",
    "X_t = pd.read_csv(data_file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb377161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to int64 data type\n",
    "X_t['date'] = X_t['date'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca0511e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only number of sensornode instead of string name\n",
    "X_t['Sensor_ID'] = X_t['Sensor_ID'].str[-2:].astype(int) # anpassen auf Zwei Stellen!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a9b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non relevant labels\n",
    "X_t = X_t.drop(columns = ['scenario_label', \n",
    "                          'progress_label', \n",
    "                          'anomaly_label', \n",
    "                          'ternary_label', \n",
    "                          #'Motion_Room', \n",
    "                          #'Motion_Room_Trend'\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb667197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "X_t = X_t.rename(columns={\"date\": \"timepoints\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927e8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_t in mtype=\"pd-multiindex\" format for sktime\n",
    "# doc.: https://github.com/sktime/sktime/blob/main/examples/AA_datatypes_and_datasets.ipynb\n",
    "\n",
    "X_t.set_index(['interval_label','Sensor_ID', 'timepoints'], inplace=True)\n",
    "\n",
    "X_t = X_t.groupby(level=[0,1,2], sort=True).sum()\n",
    "\n",
    "X_t['timepoints'] = X_t.groupby(['interval_label','Sensor_ID']).cumcount()\n",
    "X_t['timepoints'] = X_t['timepoints'].apply(lambda x: x*10)\n",
    "\n",
    "X_t = X_t.set_index('timepoints', append=True)\n",
    "X_t = X_t.droplevel(2)\n",
    "\n",
    "# define data columns\n",
    "data_columns = FEATURES\n",
    "\n",
    "# Pivot the DataFrame to have Sensor_ID levels as columns\n",
    "X_t_pivot = X_t.pivot_table(index=['interval_label', 'timepoints'], columns='Sensor_ID', values=data_columns)\n",
    "\n",
    "# Flatten the column MultiIndex\n",
    "X_t_pivot.columns = [f\"{col[0]}_{col[1]}\" for col in X_t_pivot.columns]\n",
    "\n",
    "# Give fire label if at least one sensornode in that interval takes \"fire\" label\n",
    "label_column_df = X_t.groupby('interval_label')['fire_label'].apply(lambda x: 'Fire' in x.values).map({True: 'Fire', False: 'NoFire'})\n",
    "\n",
    "# add flattend fire label to df\n",
    "X_t_pivot_merged = X_t_pivot.merge(label_column_df, how='left', left_on='interval_label', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e03720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoding and decoding of Label column\n",
    "# Encode string labels to integers\n",
    "label_mapping = {\"NoFire\": 0, \"Fire\": 1} # do the label encoding by yourself\n",
    "\n",
    "# Define a function to map labels using the dictionary\n",
    "def map_labels(label):\n",
    "    return label_mapping[label]\n",
    "\n",
    "# Inverse mapping dictionary\n",
    "inverse_label_mapping = {0: \"NoFire\", 1: \"Fire\"}\n",
    "\n",
    "# Use numpy.vectorize to apply the mapping function element-wise\n",
    "X_t_pivot_merged['fire_label'] = np.vectorize(map_labels)(X_t_pivot_merged['fire_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6383f213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CO_Room_8</th>\n",
       "      <th>CO_Room_9</th>\n",
       "      <th>CO_Room_10</th>\n",
       "      <th>CO_Room_11</th>\n",
       "      <th>CO_Room_12</th>\n",
       "      <th>CO_Room_13</th>\n",
       "      <th>CO_Room_14</th>\n",
       "      <th>CO_Room_15</th>\n",
       "      <th>CO_Room_16</th>\n",
       "      <th>H2_Room_8</th>\n",
       "      <th>...</th>\n",
       "      <th>VOC_Room_RAW_8</th>\n",
       "      <th>VOC_Room_RAW_9</th>\n",
       "      <th>VOC_Room_RAW_10</th>\n",
       "      <th>VOC_Room_RAW_11</th>\n",
       "      <th>VOC_Room_RAW_12</th>\n",
       "      <th>VOC_Room_RAW_13</th>\n",
       "      <th>VOC_Room_RAW_14</th>\n",
       "      <th>VOC_Room_RAW_15</th>\n",
       "      <th>VOC_Room_RAW_16</th>\n",
       "      <th>fire_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interval_label</th>\n",
       "      <th>timepoints</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">3761</th>\n",
       "      <th>40</th>\n",
       "      <td>4.61</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2.27</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.10</td>\n",
       "      <td>4.93</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>15.4</td>\n",
       "      <td>16.1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>16.5</td>\n",
       "      <td>10.6</td>\n",
       "      <td>14.1</td>\n",
       "      <td>15.9</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4.63</td>\n",
       "      <td>3.31</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.73</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.26</td>\n",
       "      <td>4.99</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>15.1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>14.6</td>\n",
       "      <td>16.6</td>\n",
       "      <td>10.6</td>\n",
       "      <td>14.7</td>\n",
       "      <td>15.1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4.64</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.28</td>\n",
       "      <td>5.01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>15.2</td>\n",
       "      <td>15.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>14.4</td>\n",
       "      <td>16.6</td>\n",
       "      <td>10.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4.64</td>\n",
       "      <td>3.29</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.21</td>\n",
       "      <td>4.97</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>15.3</td>\n",
       "      <td>15.7</td>\n",
       "      <td>16.4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>16.6</td>\n",
       "      <td>10.6</td>\n",
       "      <td>14.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4.62</td>\n",
       "      <td>3.31</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.27</td>\n",
       "      <td>4.95</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>15.4</td>\n",
       "      <td>15.8</td>\n",
       "      <td>16.6</td>\n",
       "      <td>14.5</td>\n",
       "      <td>16.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>15.1</td>\n",
       "      <td>13.7</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33858 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           CO_Room_8  CO_Room_9  CO_Room_10  CO_Room_11  \\\n",
       "interval_label timepoints                                                 \n",
       "0              0               -0.25      -0.43        0.15       -0.08   \n",
       "               10              -0.25      -0.43        0.15       -0.08   \n",
       "               20              -0.25      -0.43        0.15       -0.08   \n",
       "               30              -0.28      -0.30        0.14        0.03   \n",
       "               40              -0.24      -0.22        0.19        0.07   \n",
       "...                              ...        ...         ...         ...   \n",
       "3761           40               4.61       3.22        2.27        2.55   \n",
       "               50               4.63       3.31        2.36        2.67   \n",
       "               60               4.64       3.26        2.37        2.77   \n",
       "               70               4.64       3.29        2.54        2.79   \n",
       "               80               4.62       3.31        2.50        2.76   \n",
       "\n",
       "                           CO_Room_12  CO_Room_13  CO_Room_14  CO_Room_15  \\\n",
       "interval_label timepoints                                                   \n",
       "0              0                -0.16       -0.03        0.08       -0.08   \n",
       "               10               -0.16       -0.03        0.08       -0.08   \n",
       "               20               -0.16       -0.03        0.08       -0.03   \n",
       "               30               -0.13        0.04        0.10       -0.07   \n",
       "               40               -0.09       -0.07        0.00       -0.11   \n",
       "...                               ...         ...         ...         ...   \n",
       "3761           40                2.71        2.04        2.10        4.93   \n",
       "               50                2.73        2.08        2.26        4.99   \n",
       "               60                2.75        2.09        2.28        5.01   \n",
       "               70                2.78        2.18        2.21        4.97   \n",
       "               80                2.81        2.17        2.27        4.95   \n",
       "\n",
       "                           CO_Room_16  H2_Room_8  ...  VOC_Room_RAW_8  \\\n",
       "interval_label timepoints                         ...                   \n",
       "0              0                 0.18       0.00  ...             0.5   \n",
       "               10                0.18       0.00  ...             0.5   \n",
       "               20                0.01       0.00  ...             0.5   \n",
       "               30               -0.08       0.05  ...             0.5   \n",
       "               40               -0.03       0.06  ...             0.5   \n",
       "...                               ...        ...  ...             ...   \n",
       "3761           40                3.70       0.29  ...            15.4   \n",
       "               50                3.76       0.27  ...            15.1   \n",
       "               60                3.73       0.28  ...            15.2   \n",
       "               70                3.74       0.31  ...            15.3   \n",
       "               80                3.66       0.31  ...            15.4   \n",
       "\n",
       "                           VOC_Room_RAW_9  VOC_Room_RAW_10  VOC_Room_RAW_11  \\\n",
       "interval_label timepoints                                                     \n",
       "0              0                      0.5              0.6              0.5   \n",
       "               10                     0.5              0.6              0.5   \n",
       "               20                     0.5              0.6              0.5   \n",
       "               30                     0.5              0.6              0.5   \n",
       "               40                     0.5              0.6              0.5   \n",
       "...                                   ...              ...              ...   \n",
       "3761           40                    16.1             15.8             14.9   \n",
       "               50                    16.0             15.9             14.6   \n",
       "               60                    15.9             16.1             14.4   \n",
       "               70                    15.7             16.4             14.7   \n",
       "               80                    15.8             16.6             14.5   \n",
       "\n",
       "                           VOC_Room_RAW_12  VOC_Room_RAW_13  VOC_Room_RAW_14  \\\n",
       "interval_label timepoints                                                      \n",
       "0              0                       0.6              0.5              0.5   \n",
       "               10                      0.6              0.5              0.5   \n",
       "               20                      0.6              0.5              0.5   \n",
       "               30                      0.5              0.5              0.5   \n",
       "               40                      0.6              0.5              0.5   \n",
       "...                                    ...              ...              ...   \n",
       "3761           40                     16.5             10.6             14.1   \n",
       "               50                     16.6             10.6             14.7   \n",
       "               60                     16.6             10.7             15.0   \n",
       "               70                     16.6             10.6             14.9   \n",
       "               80                     16.6             10.5             15.1   \n",
       "\n",
       "                           VOC_Room_RAW_15  VOC_Room_RAW_16  fire_label  \n",
       "interval_label timepoints                                                \n",
       "0              0                       0.5              0.5           0  \n",
       "               10                      0.5              0.5           0  \n",
       "               20                      0.5              0.5           0  \n",
       "               30                      0.5              0.5           0  \n",
       "               40                      0.5              0.5           0  \n",
       "...                                    ...              ...         ...  \n",
       "3761           40                     15.9             13.0           0  \n",
       "               50                     15.1             13.0           0  \n",
       "               60                     14.3             13.2           0  \n",
       "               70                     14.0             13.3           0  \n",
       "               80                     13.7             13.2           0  \n",
       "\n",
       "[33858 rows x 46 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t_pivot_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f720e63",
   "metadata": {},
   "source": [
    "## Define model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df4f5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Explanation Function\n",
    "def mvts_shap(X_train, X_test, y_train, y_test, X_reference_negative, X_reference_positive):\n",
    "    \n",
    "    i, j, k = X_train.shape # i = intervals, j = features, k = timepoints per interval\n",
    "    u, v, w = X_test.shape\n",
    "    l, s, t = X_reference_negative.shape \n",
    "    \n",
    "    X_train_flat = X_train.reshape(i, j*k)\n",
    "    X_reference_negative_flat = X_reference_negative.reshape(l, s*t) # reshape selected background intervals\n",
    "    X_reference_positive_flat = X_reference_positive.reshape(l, s*t)\n",
    "    X_reference_positive_and_negative_flat = np.concatenate((X_reference_negative_flat, X_reference_positive_flat), axis=0)\n",
    "    \n",
    "    def reshaper(inner_tensor):\n",
    "        return inner_tensor.reshape(inner_tensor.shape[0], j, k)\n",
    "\n",
    "    def inv_reshaper(inner_tensor):\n",
    "        return inner_tensor.reshape(inner_tensor.shape[0], j*k)\n",
    "\n",
    "    param = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'tree_method': 'hist',\n",
    "            'eval_metric': 'logloss',\n",
    "            'seed': 888,\n",
    "            'n_estimators': 500\n",
    "           }\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('reshaper_t', FunctionTransformer(reshaper, inverse_func=inv_reshaper)),\n",
    "        ('tabulariser', MiniRocketMultivariate(num_kernels=10000, n_jobs= -1, random_state=1837)), #(num_kernels=588, n_jobs=-1, random_state=1837)\n",
    "        ('scaler', StandardScaler()), #'scaler', StandardScaler() # 'scaler', MinMaxScaler()\n",
    "        ('bst', xgb.XGBClassifier(**param))\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train_flat, y_train)\n",
    "    \n",
    "    # Take n random samples from referenz data\n",
    "    masker = shap.maskers.Independent(X_reference_positive_and_negative_flat, 6) # number defines the amount of intervals to be considered\n",
    "    #X_reference_negative_flat #X_reference_positive_flat #3\n",
    "    \n",
    "    # fixed background dataset with the same shape as X_train_flat_summary\n",
    "    fixed_background_data = np.full_like(masker.data, 1000)\n",
    "    \n",
    "    # Define Explainer\n",
    "    explainer = shap.KernelExplainer(pipe.predict_proba, masker.data)\n",
    "\n",
    "    # Explanation on 0 to n = sample_size intervals\n",
    "    sample_size =  u \n",
    "    sample_idx = np.arange(sample_size)\n",
    "    \n",
    "    # Get the model's predictions on the test data\n",
    "    start_time = time.time() # start runtime\n",
    "    model_predictions_encoded = pipe.predict(X_test.reshape(u, v*w))\n",
    "    end_time = time.time() # end runtime\n",
    "    prediction_runtime = end_time - start_time # calculate runtime of prediction component\n",
    "    print(f\"Prediction runtime: {prediction_runtime:.4f} seconds\")\n",
    "    \n",
    "    # Get predict_proba on test data\n",
    "    model_predict_proba = pipe.predict_proba(X_test.reshape(u, v*w))\n",
    "    \n",
    "    # Derive SHAP explanations Explainer\n",
    "    shap_output = explainer.shap_values(X_test[sample_idx, :, :].reshape(sample_size, v * w))    \n",
    "    shap_tensor_no_fire = shap_output[0].reshape(sample_size, v, w) #ACHTUNG: [0] muss encoded dem gewünschten Label entsprechen!\n",
    "    shap_tensor_fire = shap_output[1].reshape(sample_size, v, w) \n",
    "    base_value_shap = explainer.expected_value[1]\n",
    "        \n",
    "    return shap_tensor_no_fire, shap_tensor_fire, model_predictions_encoded, model_predict_proba, base_value_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6618401",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11384876",
   "metadata": {},
   "source": [
    "### Train/ Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2619777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = X_t_pivot_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dca921a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive Train and hold-out (test) set\n",
    "split_interval = 720\n",
    "\n",
    "# Split Time Series into train and test based on split interval\n",
    "df_train = train_test_data.loc[train_test_data.index.get_level_values('interval_label') <= split_interval]\n",
    "df_test = train_test_data.loc[train_test_data.index.get_level_values('interval_label') > split_interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2233455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive X_train, X_test etc. \n",
    "feature_columns = train_test_data.columns.difference(['fire_label'])\n",
    "# X_train\n",
    "X_train = df_train[feature_columns]\n",
    "# X_test\n",
    "X_test = df_test[feature_columns]\n",
    "# y_train\n",
    "y_train = df_train['fire_label'].groupby('interval_label').first().to_numpy()\n",
    "# y_test\n",
    "y_test = df_test['fire_label'].groupby('interval_label').first().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9955d",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2fd93eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction runtime: 0.6753 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd7362243d34eb1a1a3a5d04bbcdc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m     X_reference_positive_3d \u001b[38;5;241m=\u001b[39m convert_to(X_background_shap_positive, to_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy3D\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# apply mvts_shap function to data\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     shap_values_no_fire, shap_values_fire, y_pred_encoded, pred_proba, base_value_shap \u001b[38;5;241m=\u001b[39m \u001b[43mmvts_shap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m                                                                                                   \u001b[49m\u001b[43mX_test_3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m                                                                                                   \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m                                                                                                   \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                                                                                                   \u001b[49m\u001b[43mX_reference_negative_3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m                                                                                                   \u001b[49m\u001b[43mX_reference_positive_3d\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m                                                                                                  \u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Direct Print outs\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScaler:\u001b[39m\u001b[38;5;124m'\u001b[39m, scaler_name)\n",
      "Cell \u001b[1;32mIn[14], line 61\u001b[0m, in \u001b[0;36mmvts_shap\u001b[1;34m(X_train, X_test, y_train, y_test, X_reference_negative, X_reference_positive)\u001b[0m\n\u001b[0;32m     58\u001b[0m model_predict_proba \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict_proba(X_test\u001b[38;5;241m.\u001b[39mreshape(u, v\u001b[38;5;241m*\u001b[39mw))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Derive SHAP explanations Explainer\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m shap_output \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     62\u001b[0m shap_tensor_no_fire \u001b[38;5;241m=\u001b[39m shap_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(sample_size, v, w) \u001b[38;5;66;03m#ACHTUNG: [0] muss encoded dem gewünschten Label entsprechen!\u001b[39;00m\n\u001b[0;32m     63\u001b[0m shap_tensor_fire \u001b[38;5;241m=\u001b[39m shap_output[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(sample_size, v, w) \n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\shap\\explainers\\_kernel.py:242\u001b[0m, in \u001b[0;36mKernel.shap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[0;32m    241\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[1;32m--> 242\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    244\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\shap\\explainers\\_kernel.py:442\u001b[0m, in \u001b[0;36mKernel.explain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m phi_var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD):\n\u001b[1;32m--> 442\u001b[0m     vphi, vphi_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     phi[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaryingInds, d] \u001b[38;5;241m=\u001b[39m vphi\n\u001b[0;32m    444\u001b[0m     phi_var[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaryingInds, d] \u001b[38;5;241m=\u001b[39m vphi_var\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\shap\\explainers\\_kernel.py:626\u001b[0m, in \u001b[0;36mKernel.solve\u001b[1;34m(self, fraction_evaluated, dim)\u001b[0m\n\u001b[0;32m    624\u001b[0m         kwg \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    625\u001b[0m     model \u001b[38;5;241m=\u001b[39m make_pipeline(StandardScaler(with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), LassoLarsIC(criterion\u001b[38;5;241m=\u001b[39mc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwg))\n\u001b[1;32m--> 626\u001b[0m     nonzero_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meyAdj_aug\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcoef_)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# use a fixed regularization coeffcient\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    630\u001b[0m     nonzero_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(Lasso(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_reg)\u001b[38;5;241m.\u001b[39mfit(mask_aug, eyAdj_aug)\u001b[38;5;241m.\u001b[39mcoef_)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\sklearn\\linear_model\\_least_angle.py:2299\u001b[0m, in \u001b[0;36mLassoLarsIC.fit\u001b[1;34m(self, X, y, copy_X)\u001b[0m\n\u001b[0;32m   2296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_ \u001b[38;5;241m=\u001b[39m alphas_\n\u001b[0;32m   2298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_variance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_variance_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_noise_variance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\n\u001b[0;32m   2301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_variance_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_variance\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\sklearn\\linear_model\\_least_angle.py:2348\u001b[0m, in \u001b[0;36mLassoLarsIC._estimate_noise_variance\u001b[1;34m(self, X, y, positive)\u001b[0m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;66;03m# X and y are already centered and we don't need to fit with an intercept\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m ols_model \u001b[38;5;241m=\u001b[39m LinearRegression(positive\u001b[38;5;241m=\u001b[39mpositive, fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 2348\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mols_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m   2349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum((y \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\n\u001b[0;32m   2350\u001b[0m     X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept\n\u001b[0;32m   2351\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\sklearn\\linear_model\\_base.py:665\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    662\u001b[0m )\n\u001b[0;32m    664\u001b[0m \u001b[38;5;66;03m# Sample weight can be implemented via a simple rescaling.\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m X, y, sample_weight_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43m_rescale_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\sklearn\\linear_model\\_base.py:322\u001b[0m, in \u001b[0;36m_rescale_data\u001b[1;34m(X, y, sample_weight)\u001b[0m\n\u001b[0;32m    320\u001b[0m sample_weight_sqrt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(sample_weight)\n\u001b[0;32m    321\u001b[0m sw_matrix \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mdia_matrix((sample_weight_sqrt, \u001b[38;5;241m0\u001b[39m), shape\u001b[38;5;241m=\u001b[39m(n_samples, n_samples))\n\u001b[1;32m--> 322\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43msw_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m y \u001b[38;5;241m=\u001b[39m safe_sparse_dot(sw_matrix, y)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, sample_weight_sqrt\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\sklearn\\utils\\extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\scipy\\sparse\\_base.py:630\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar operands are not allowed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\scipy\\sparse\\_base.py:532\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_vector(other\u001b[38;5;241m.\u001b[39mravel())\u001b[38;5;241m.\u001b[39mreshape(M, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m N:\n\u001b[1;32m--> 532\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_multivector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_scalar(other)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\scipy\\sparse\\_base.py:600\u001b[0m, in \u001b[0;36mspmatrix._mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mul_multivector\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtocsr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_multivector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DECODE\\lib\\site-packages\\scipy\\sparse\\_compressed.py:502\u001b[0m, in \u001b[0;36m_cs_matrix._mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[0;32m    501\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_matvecs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m   \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate background referenz data for SHAP explanation\n",
    "# Define number of intervals for shap referenz data\n",
    "sample_size = 5\n",
    "\n",
    "dfs_max_shap_values_fire = []\n",
    "dfs_shap_mean = []\n",
    "\n",
    "# Randomly select n rows where \"NoFire\" occurs in the \"fire_label\" column based on 'Interval_label'\n",
    "# Find all intervals where 'fire_label' is 'NoFire' for each group \n",
    "selected_intervals_negative = list(df_train.loc[df_train['fire_label'] == 0].groupby(level='interval_label').groups.keys())\n",
    "selected_intervals_positive = list(df_train.loc[df_train['fire_label'] == 1].groupby(level='interval_label').groups.keys())\n",
    "# sample n intervals\n",
    "selected_intervals_negative = np.random.choice(selected_intervals_negative, size=min(sample_size, len(selected_intervals_negative)), replace=False)\n",
    "selected_intervals_positive = np.random.choice(selected_intervals_positive, size=min(sample_size, len(selected_intervals_positive)), replace=False)\n",
    "\n",
    "# Scaling: static for train and test set, not per subsequence\n",
    "# Define Scaler\n",
    "dict_scaler = {'NoScaling': None,\n",
    "#                'QuantileTransformer10': QuantileTransformer(n_quantiles=10, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer20': QuantileTransformer(n_quantiles=20, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer30': QuantileTransformer(n_quantiles=30, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer5': QuantileTransformer(n_quantiles=5, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer4': QuantileTransformer(n_quantiles=4, random_state=0, output_distribution='normal'),\n",
    "#                'QuantileTransformer50': QuantileTransformer(n_quantiles=50, random_state=0, output_distribution='normal'),\n",
    "#                'RobustScaler': RobustScaler(),\n",
    "#                'MaxAbsScaler': MaxAbsScaler(),\n",
    "#                'PowerTransformer': PowerTransformer(),\n",
    "#                'Normalizer': Normalizer(),\n",
    "#                'StandardScaler': StandardScaler(),\n",
    "#                'MinMaxScaler': MinMaxScaler()\n",
    "              }\n",
    "\n",
    "for scaler_name, scaler_instance in dict_scaler.items():\n",
    "\n",
    "    if scaler_instance is not None:\n",
    "        # Define scaler\n",
    "        scaler = scaler_instance\n",
    "\n",
    "        # Fit the scaler on X_train\n",
    "        scaler.fit(X_train)\n",
    "\n",
    "        # Transform X_train and X_test separately\n",
    "        X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "        # Use pd.IndexSlice to select rows based on the array of indexes\n",
    "        X_background_shap_negative_scaled = X_train_scaled.loc[pd.IndexSlice[selected_intervals_negative, :], :]\n",
    "        X_background_shap_positive_scaled = X_train_scaled.loc[pd.IndexSlice[selected_intervals_positive, :], :]\n",
    "\n",
    "        # Transform X_train etc. into 3D numpy array\n",
    "        X_train_3d_scaled = convert_to(X_train_scaled, to_type=\"numpy3D\")\n",
    "        X_test_3d_scaled = convert_to(X_test_scaled, to_type=\"numpy3D\")\n",
    "        X_reference_negative_3d_scaled = convert_to(X_background_shap_negative_scaled, to_type=\"numpy3D\")\n",
    "        X_reference_positive_3d_scaled = convert_to(X_background_shap_positive_scaled, to_type=\"numpy3D\")                \n",
    "\n",
    "        # apply mvts_shap function to data\n",
    "        shap_values_no_fire, shap_values_fire, y_pred_encoded, pred_proba, base_value_shap = mvts_shap(X_train_3d_scaled,\n",
    "                                                                                                       X_test_3d_scaled,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       y_test,\n",
    "                                                                                                       X_reference_negative_3d_scaled,\n",
    "                                                                                                       X_reference_positive_3d_scaled\n",
    "                                                                                                      )\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Use pd.IndexSlice to select rows based on the array of indexes\n",
    "        X_background_shap_negative = X_train.loc[pd.IndexSlice[selected_intervals_negative, :], :]\n",
    "        X_background_shap_positive = X_train.loc[pd.IndexSlice[selected_intervals_positive, :], :]\n",
    "\n",
    "        # Transform X_train etc. into 3D numpy array (unscaled)\n",
    "        X_train_3d = convert_to(X_train, to_type=\"numpy3D\")\n",
    "        X_test_3d = convert_to(X_test, to_type=\"numpy3D\")\n",
    "        X_reference_negative_3d = convert_to(X_background_shap_negative, to_type=\"numpy3D\")\n",
    "        X_reference_positive_3d = convert_to(X_background_shap_positive, to_type=\"numpy3D\")\n",
    "\n",
    "        # apply mvts_shap function to data\n",
    "        shap_values_no_fire, shap_values_fire, y_pred_encoded, pred_proba, base_value_shap = mvts_shap(X_train_3d,\n",
    "                                                                                                       X_test_3d,\n",
    "                                                                                                       y_train,\n",
    "                                                                                                       y_test,\n",
    "                                                                                                       X_reference_negative_3d,\n",
    "                                                                                                       X_reference_positive_3d\n",
    "                                                                                                      ) \n",
    "        \n",
    "    # Direct Print outs\n",
    "    print('Scaler:', scaler_name)\n",
    "    print('max Shap value NoFire: ', shap_values_no_fire.max())\n",
    "    print('min Shap value NoFire', shap_values_no_fire.min())\n",
    "    print('max Shap value Fire: ', shap_values_fire.max())\n",
    "    print('min Shap value Fire', shap_values_fire.min())\n",
    "    print('Base SHAP Value (expected_value):', base_value_shap)\n",
    "    \n",
    "    # Inverse transform the encoded labels to decode them\n",
    "    y_pred = np.vectorize(inverse_label_mapping.get)(y_pred_encoded)\n",
    "\n",
    "    # Retransform from 3D array to multiindex df\n",
    "    # convert\n",
    "    df_shap_values_no_fire = convert(shap_values_no_fire, from_type=\"numpy3D\", to_type=\"pd-multiindex\")\n",
    "    df_shap_values_fire = convert(shap_values_fire, from_type=\"numpy3D\", to_type=\"pd-multiindex\")             \n",
    "    # Get column names based on original df\n",
    "    columns_names = X_train.columns.tolist()\n",
    "    # Rename the columns\n",
    "    df_shap_values_no_fire.columns = columns_names\n",
    "    df_shap_values_fire.columns = columns_names\n",
    "    # Multiply the values in the second index level by 10\n",
    "    df_shap_values_no_fire.index = df_shap_values_no_fire.index.set_levels(df_shap_values_no_fire.index.levels[1] * 10, level=1)\n",
    "    df_shap_values_fire.index = df_shap_values_fire.index.set_levels(df_shap_values_fire.index.levels[1] * 10, level=1)\n",
    "    # rename index\n",
    "    df_shap_values_no_fire.index = df_shap_values_no_fire.index.set_names('interval_label', level=0)\n",
    "    df_shap_values_fire.index = df_shap_values_fire.index.set_names('interval_label', level=0)\n",
    "    \n",
    "    # speichere die max_shap_values pro feature und Sensorknoten für fire class\n",
    "    df_max_shap_values_fire_temp = pd.DataFrame(df_shap_values_fire.max()).reset_index().rename(columns={0: 'shap_value_fire_max', 'index': 'feature'})\n",
    "    dfs_max_shap_values_fire.append(df_max_shap_values_fire_temp)\n",
    "    \n",
    "\n",
    "    # Add model Predictions to df_test \n",
    "    # Convert the NumPy array to a pandas DataFrame\n",
    "    y_pred_df = pd.DataFrame({'model_prediction': y_pred})\n",
    "\n",
    "    # Add a second column containing the interval_label\n",
    "    y_pred_df['interval_label'] = df_test.index.get_level_values('interval_label').unique()\n",
    "\n",
    "    # Set 'index_column' as the index\n",
    "    y_pred_df.set_index('interval_label', inplace=True)\n",
    "\n",
    "    # Convert the DataFrame to a dictionary\n",
    "    y_pred_dict = y_pred_df.to_dict(orient='dict')['model_prediction']\n",
    "\n",
    "    # add model_prediction to df_test_export\n",
    "    df_test_export = df_test.copy()\n",
    "    df_test_export['model_prediction'] = df_test_export.index.get_level_values('interval_label').map(y_pred_dict)\n",
    "\n",
    "    # encode the true label in df_test_export\n",
    "    df_test_export['fire_label'] = np.vectorize(inverse_label_mapping.get)(df_test_export['fire_label'])\n",
    "\n",
    "    # Reset Interval_label index so that it can be merged with df_shap_values\n",
    "    # Calculate the offset to align df_2 with df_1\n",
    "    offset = df_shap_values_no_fire.index.get_level_values('interval_label').min() - df_test_export.index.get_level_values('interval_label').min()\n",
    "    # Adjust the index of df_2\n",
    "    df_test_export.index = df_test_export.index.set_levels(df_test_export.index.levels[0] + offset, level=0)\n",
    "\n",
    "    # Export df_test\n",
    "    # Construct the absolute path to the data file\n",
    "    file_name = f'df_test_elba_rocket_network_{scaler_name}.csv'\n",
    "    data_file_path = os.path.join(directory_export, file_name)\n",
    "    df_test_export.to_csv(data_file_path)\n",
    "\n",
    "    # Export df_shap_values\n",
    "    file_name = f'df_shap_values_elba_rocket_nofire_network_{scaler_name}.csv'\n",
    "    data_file_path = os.path.join(directory_export, file_name)\n",
    "    df_shap_values_no_fire.to_csv(data_file_path) \n",
    "\n",
    "    # Export df_shap_values\n",
    "    file_name = f'df_shap_values_elba_rocket_fire_network_{scaler_name}.csv'\n",
    "    data_file_path = os.path.join(directory_export, file_name)\n",
    "    df_shap_values_fire.to_csv(data_file_path) \n",
    "    \n",
    "    # Define SHAP Plots\n",
    "    # Define column order for facetgrid plots\n",
    "    col_order = ['8',\n",
    "                 '9', \n",
    "                 '10',\n",
    "                 '11',\n",
    "                 '12',\n",
    "                 '13',\n",
    "                 '14',\n",
    "                 '15',\n",
    "                 '16']\n",
    "    \n",
    "    # summarize timepoints per interval by mean\n",
    "    df_shap_values_fire_mean = df_shap_values_fire.copy()\n",
    "    df_shap_values_fire_mean = df_shap_values_fire_mean.groupby(level='interval_label').mean()\n",
    "    # write label for fire resp. no_fire intervals\n",
    "    df_shap_values_fire_mean['fire_label'] = df_test_export.groupby(level='interval_label')['fire_label'].first() #'fire'\n",
    "    \n",
    "    # derive max absolute values by respecting the sign (fire and no_fire intervals)\n",
    "    # create new df to derive indexes\n",
    "    df_indexes = df_shap_values_fire.copy()\n",
    "    # find indexes of maximum absolute value \n",
    "    df_indexes = df_indexes.groupby('interval_label').apply(lambda group: group.loc[:, group.columns != 'fire_label'].abs().idxmax())\n",
    "    # replace indexes by shap values\n",
    "    df_shap_values_fire_max_abs_sign = df_indexes.copy()\n",
    "    for col in df_indexes.columns:\n",
    "        df_shap_values_fire_max_abs_sign[col] = [df_shap_values_fire.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "    # replace indexes by measurement values\n",
    "    df_measurement_values_fire_max_abs_sign = df_indexes.copy()\n",
    "    for col in df_indexes.columns:\n",
    "        df_measurement_values_fire_max_abs_sign[col] = [df_test_export.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "    # derive scaled measurement values for coloring feature size in plot\n",
    "    columns_to_bin = df_measurement_values_fire_max_abs_sign.columns\n",
    "    df_measurement_values_fire_max_abs_sign_scaled = df_measurement_values_fire_max_abs_sign.copy()\n",
    "    # Define the number of bins (adjust as needed)\n",
    "    num_bins = 5\n",
    "    # Loop through each column and perform binning, scaling, and conversion to integers\n",
    "    for column in columns_to_bin:\n",
    "        # Create bins using cut\n",
    "        df_measurement_values_fire_max_abs_sign_scaled[column] = pd.cut(df_measurement_values_fire_max_abs_sign_scaled[column], bins=num_bins, labels=False)\n",
    "        # Scale the values to the range [0, 10]\n",
    "        df_measurement_values_fire_max_abs_sign_scaled[column] = df_measurement_values_fire_max_abs_sign_scaled[column] * ((5 - 1) / (num_bins - 1)) + 1 \n",
    "        # Convert scaled values to integers\n",
    "        df_measurement_values_fire_max_abs_sign_scaled[column] = df_measurement_values_fire_max_abs_sign_scaled[column].astype(int)  \n",
    "    # melt the scaled df in the same form as the shap value for plot\n",
    "    df_measurement_values_fire_max_abs_sign_scaled_melted = pd.melt(df_measurement_values_fire_max_abs_sign_scaled, var_name='feature', value_name='size')\n",
    "    df_measurement_values_fire_max_abs_sign_scaled_melted['sensornode'] = df_measurement_values_fire_max_abs_sign_scaled_melted['feature'].apply(lambda x: x.split('_')[-1])\n",
    "    df_measurement_values_fire_max_abs_sign_scaled_melted['feature'] = df_measurement_values_fire_max_abs_sign_scaled_melted['feature'].apply(lambda x: x.split('_')[0])\n",
    "    # melt shap values df\n",
    "    df_melted = pd.melt(df_shap_values_fire_max_abs_sign, var_name='feature', value_name='shap_value')\n",
    "    # swarmplot single node\n",
    "    df_plot = df_melted.copy()\n",
    "    df_plot['sensornode'] = df_plot['feature'].apply(lambda x: x.split('_')[-1])\n",
    "    df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "    # add feature size to df plot\n",
    "    # df1.merge(df2, how='inner', on='a') # (alternative)\n",
    "    list_feature_sizes = df_measurement_values_fire_max_abs_sign_scaled_melted['size'].to_list()\n",
    "    df_plot['size'] = list_feature_sizes\n",
    "    # Create a FacetGrid\n",
    "    feature_order = ['CO','H2','PM05','PM10','VOC']\n",
    "    g = sns.FacetGrid(data=df_plot, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, hue = 'size', palette=\"flare\") #, hue = 'size'\n",
    "    # Map a swarmplot to the FacetGrid\n",
    "    g.map(sns.stripplot, \"shap_value\", \"feature\", orient=\"h\", alpha=.8, s=2, order=feature_order) #swarmplot\n",
    "    g.map(sns.violinplot, \"shap_value\", \"feature\", inner=None, color=\"lightgray\", linewidth=0.5, order=feature_order)\n",
    "    # Set axis labels and adjust plot layout\n",
    "    # g.add_legend(title = 'feature value')\n",
    "    # Customize x-axis labels for every subplot\n",
    "    # g.set_xticklabels()\n",
    "    # Add legend\n",
    "    g.add_legend(title='feature value')\n",
    "    # axes titles\n",
    "    g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "    # Save the plot to a PNG file\n",
    "    file_name = f'shap_values_max_abs_sign_network_swarmplot_{scaler_name}.png'\n",
    "    # Create the file path using an f-string\n",
    "    file_path = os.path.join(directory_export, file_name)\n",
    "    plt.savefig(file_path)\n",
    "    \n",
    "    \n",
    "    # derive max absolute SHAP values by respecting the sign (only fire intervals)\n",
    "    # create new df to derive indexes\n",
    "    df_indexes = df_shap_values_fire.copy()\n",
    "    # add fire_label to select only fire intervals\n",
    "    df_indexes['fire_label'] = df_test_export['fire_label'].to_list()[:len(df_shap_values_fire)]\n",
    "    # select only fire intervals\n",
    "    df_indexes = df_indexes.loc[df_indexes.fire_label == 'Fire'] # Fire\n",
    "    # find indexes of maximum absolute value \n",
    "    df_indexes = df_indexes.groupby('interval_label').apply(lambda group: group.loc[:, group.columns != 'fire_label'].abs().idxmax())\n",
    "    # replace indexes by shap values\n",
    "    df_shap_values_fire_max_abs_sign = df_indexes.copy()\n",
    "    for col in df_indexes.columns:\n",
    "        df_shap_values_fire_max_abs_sign[col] = [df_shap_values_fire.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "    # replace indexes by measurement values\n",
    "    df_measurement_values_fire_max_abs_sign = df_indexes.copy()\n",
    "    for col in df_indexes.columns:\n",
    "        df_measurement_values_fire_max_abs_sign[col] = [df_test_export.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "    # derive scaled measurement values for coloring feature size in plot\n",
    "    columns_to_bin = df_measurement_values_fire_max_abs_sign.columns\n",
    "    df_measurement_values_fire_max_abs_sign_scaled = df_measurement_values_fire_max_abs_sign.copy()\n",
    "    # Define the number of bins (adjust as needed)\n",
    "    num_bins = 5\n",
    "    # Loop through each column and perform binning, scaling, and conversion to integers\n",
    "    for column in columns_to_bin:\n",
    "        # Create bins using cut\n",
    "        df_measurement_values_fire_max_abs_sign_scaled[column] = pd.cut(df_measurement_values_fire_max_abs_sign_scaled[column], bins=num_bins, labels=False)\n",
    "        # Scale the values to the range [0, 10]\n",
    "        df_measurement_values_fire_max_abs_sign_scaled[column] = df_measurement_values_fire_max_abs_sign_scaled[column] * ((5 - 1) / (num_bins - 1)) + 1 # (5 / (num_bins - 1))\n",
    "        # Convert scaled values to integers\n",
    "        df_measurement_values_fire_max_abs_sign_scaled[column] = df_measurement_values_fire_max_abs_sign_scaled[column].astype(int)  \n",
    "    # melt the scaled df in the same form as the shap value for plot\n",
    "    df_measurement_values_fire_max_abs_sign_scaled_melted = pd.melt(df_measurement_values_fire_max_abs_sign_scaled, var_name='feature', value_name='size')\n",
    "    df_measurement_values_fire_max_abs_sign_scaled_melted['sensornode'] = df_measurement_values_fire_max_abs_sign_scaled_melted['feature'].apply(lambda x: x.split('_')[-1])\n",
    "    df_measurement_values_fire_max_abs_sign_scaled_melted['feature'] = df_measurement_values_fire_max_abs_sign_scaled_melted['feature'].apply(lambda x: x.split('_')[0])\n",
    "    # melt shap values df\n",
    "    df_melted = pd.melt(df_shap_values_fire_max_abs_sign, var_name='feature', value_name='shap_value')\n",
    "    # swarmplot single node\n",
    "    df_plot = df_melted.copy()\n",
    "    df_plot['sensornode'] = df_plot['feature'].apply(lambda x: x.split('_')[-1])\n",
    "    df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "    # add feature size to df plot\n",
    "    # df1.merge(df2, how='inner', on='a') # (alternative)\n",
    "    list_feature_sizes = df_measurement_values_fire_max_abs_sign_scaled_melted['size'].to_list()\n",
    "    df_plot['size'] = list_feature_sizes\n",
    "    # Create a FacetGrid\n",
    "    feature_order = ['CO','H2','PM05','PM10','VOC']\n",
    "    g = sns.FacetGrid(data=df_plot, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, hue = 'size', palette=\"flare\") #, hue = 'size'\n",
    "    # Map a swarmplot to the FacetGrid\n",
    "    g.map(sns.stripplot, \"shap_value\", \"feature\", orient=\"h\", alpha=.8, s=2, order=feature_order) #swarmplot\n",
    "    g.map(sns.violinplot, \"shap_value\", \"feature\", inner=None, color=\"lightgray\", linewidth=0.5, order=feature_order)\n",
    "    # Add legend\n",
    "    g.add_legend(title='feature value')\n",
    "    # axes titles\n",
    "    g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "    # Save the plot to a PNG file\n",
    "    file_name = f'shap_values_max_abs_sign_only_fire_intervals_network_swarmplot_{scaler_name}.png'\n",
    "    # Create the file path using an f-string\n",
    "    file_path = os.path.join(directory_export, file_name)\n",
    "    plt.savefig(file_path)  \n",
    "\n",
    "    \n",
    "    # summarize timepoints per interval by max absolute shap values ignoring sign (fire and no_fire intervals)\n",
    "    # create new df to derive indexes\n",
    "    df_indexes = df_shap_values_fire.copy()\n",
    "    # find indexes of maximum absolute value \n",
    "    df_indexes = df_indexes.groupby('interval_label').apply(lambda group: group.loc[:, group.columns != 'fire_label'].abs().idxmax())\n",
    "    # replace indexes by shap values\n",
    "    df_shap_values_fire_max_abs = df_indexes.copy()\n",
    "    for col in df_indexes.columns:\n",
    "        df_shap_values_fire_max_abs[col] = [np.abs(df_shap_values_fire.loc[tuple(val), col]) for val in df_indexes[col]]\n",
    "    # replace indexes by measurement values\n",
    "    df_measurement_values_fire_max_abs = df_indexes.copy()\n",
    "    for col in df_indexes.columns:\n",
    "        df_measurement_values_fire_max_abs[col] = [df_test_export.loc[tuple(val), col] for val in df_indexes[col]]\n",
    "    # define columns to bin \n",
    "    columns_to_bin = df_measurement_values_fire_max_abs.columns\n",
    "    df_measurement_values_fire_max_abs_scaled = df_measurement_values_fire_max_abs.copy()\n",
    "    # Define the number of bins (adjust as needed)\n",
    "    num_bins = 5\n",
    "    # Loop through each column and perform binning, scaling, and conversion to integers\n",
    "    for column in columns_to_bin:\n",
    "        # Create bins using cut\n",
    "        df_measurement_values_fire_max_abs_scaled[column] = pd.cut(df_measurement_values_fire_max_abs_scaled[column], bins=num_bins, labels=False)\n",
    "        # Scale the values to the range [0, 10]\n",
    "        df_measurement_values_fire_max_abs_scaled[column] = df_measurement_values_fire_max_abs_scaled[column] * ((5 - 1) / (num_bins - 1)) + 1 # (5 / (num_bins - 1))\n",
    "        # Convert scaled values to integers\n",
    "        df_measurement_values_fire_max_abs_scaled[column] = df_measurement_values_fire_max_abs_scaled[column].astype(int)  \n",
    "    # melt the scaled df in the same form as the shap value for plot\n",
    "    df_measurement_values_fire_max_abs_scaled_melted = pd.melt(df_measurement_values_fire_max_abs_scaled, var_name='feature', value_name='size')\n",
    "    df_measurement_values_fire_max_abs_scaled_melted['sensornode'] = df_measurement_values_fire_max_abs_scaled_melted['feature'].apply(lambda x: x.split('_')[-1])\n",
    "    df_measurement_values_fire_max_abs_scaled_melted['feature'] = df_measurement_values_fire_max_abs_scaled_melted['feature'].apply(lambda x: x.split('_')[0])  \n",
    "    # melt shap values df\n",
    "    df_melted = pd.melt(df_shap_values_fire_max_abs, var_name='feature', value_name='shap_value')\n",
    "    # swarmplot single node\n",
    "    df_plot = df_melted.copy()\n",
    "    df_plot['sensornode'] = df_plot['feature'].apply(lambda x: x.split('_')[-1])\n",
    "    df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "    # add feature size to df plot\n",
    "    # df1.merge(df2, how='inner', on='a') # (alternative)\n",
    "    list_feature_sizes = df_measurement_values_fire_max_abs_scaled_melted['size'].to_list()\n",
    "    df_plot['size'] = list_feature_sizes\n",
    "    # Create a FacetGrid\n",
    "    feature_order = ['CO','H2','PM05','PM10','VOC']\n",
    "    g = sns.FacetGrid(data=df_plot, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, hue = 'size', palette=\"flare\") #, hue = 'size'\n",
    "    # Map a swarmplot to the FacetGrid\n",
    "    g.map(sns.stripplot, \"shap_value\", \"feature\", orient=\"h\", alpha=.8, s=2, order=feature_order) #swarmplot\n",
    "    g.map(sns.violinplot, \"shap_value\", \"feature\", inner=None, color=\"lightgray\", linewidth=0.5, order=feature_order)\n",
    "    # Add legend\n",
    "    g.add_legend(title='feature value')\n",
    "    # axes titles\n",
    "    g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "    # Save the plot to a PNG file\n",
    "    file_name = f'shap_values_max_abs_no_sign_network_swarmplot_{scaler_name}.png'\n",
    "    # Create the file path using an f-string\n",
    "    file_path = os.path.join(directory_export, file_name)\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # summarize timepoints per interval by max absolute mean shap values\n",
    "    df_shap_values_fire_max_abs_mean = df_shap_values_fire.copy()\n",
    "    df_shap_values_fire_max_abs_mean = df_shap_values_fire_max_abs_mean.groupby(level='interval_label').max().abs().mean().to_frame() #.max().abs()\n",
    "    df_shap_values_fire_max_abs_mean = df_shap_values_fire_max_abs_mean.reset_index().rename(columns = {0: \"shap_value\", 'index':'feature'})\n",
    "    # baplot single node\n",
    "    df_plot = df_shap_values_fire_max_abs_mean.copy()\n",
    "    df_plot['sensornode'] = df_plot['feature'].apply(lambda x: x.split('_')[-1])\n",
    "    df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "    df_plot = df_plot.sort_values(by=['shap_value', 'feature'], ascending=False)\n",
    "    # Create a FacetGrid\n",
    "    g = sns.FacetGrid(data=df_plot, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, sharey=False)\n",
    "    # Map a swarmplot to the FacetGrid\n",
    "    g.map(sns.barplot, \"shap_value\", \"feature\", orient=\"h\", alpha=.8)\n",
    "    # Set axis labels and adjust plot layout\n",
    "    g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "    plt.tight_layout()\n",
    "    # Save the plot to a PNG file\n",
    "    file_name = f'shap_values_max_abs_mean_network_barplot_{scaler_name}.png'\n",
    "    # Create the file path using an f-string\n",
    "    file_path = os.path.join(directory_export, file_name)\n",
    "    plt.savefig(file_path)\n",
    "    \n",
    "    # summarize timepoints per interval by max absolute mean shap values class based\n",
    "    df_shap_values_fire_max_abs_mean_classes = df_shap_values_fire.copy()\n",
    "    df_shap_values_fire_max_abs_mean_classes['fire_label'] = df_test_export['fire_label'].to_list()[:len(df_shap_values_fire)]\n",
    "    df_fire_and_interval_labels_temp = df_shap_values_fire_max_abs_mean_classes[['fire_label']].copy()\n",
    "    list_fire_and_interval_labels_temp = df_fire_and_interval_labels_temp.groupby(level = 'interval_label')['fire_label'].first().to_list()\n",
    "    df_shap_values_fire_max_abs_mean_classes = df_shap_values_fire_max_abs_mean_classes.groupby(level = 'interval_label').apply(lambda group: group.loc[:, group.columns != 'fire_label'].max().abs())\n",
    "    # add fire_label\n",
    "    df_shap_values_fire_max_abs_mean_classes['fire_label'] = list_fire_and_interval_labels_temp\n",
    "    # Group by 'fire_label' and calculate the mean for each feature\n",
    "    df_shap_values_fire_max_abs_mean_classes = df_shap_values_fire_max_abs_mean_classes.groupby('fire_label').mean()\n",
    "    # reset index\n",
    "    df_shap_values_fire_max_abs_mean_classes = df_shap_values_fire_max_abs_mean_classes.reset_index(drop=False)\n",
    "    # # df_shap_values_fire_max_abs_mean_classes.set_index('fire_label').T.plot(kind='bar', stacked=True) #alternative für stacked plot\n",
    "    # barplot single node\n",
    "    df_plot = df_shap_values_fire_max_abs_mean_classes.copy()\n",
    "    df_plot = pd.melt(df_plot, id_vars=['fire_label'], var_name='feature', value_name='shap_value')\n",
    "    df_plot['sensornode'] = df_plot['feature'].apply(lambda x: x.split('_')[-1])\n",
    "    df_plot['feature'] = df_plot['feature'].apply(lambda x: x.split('_')[0])\n",
    "    # df_plot = df_plot.sort_values(by = 'shap_value', ascending=False)\n",
    "    df_plot = df_plot.sort_values(by=['shap_value', 'feature'], ascending=False)\n",
    "    # Create a FacetGrid with Seaborn\n",
    "    g = sns.FacetGrid(data = df_plot, col=\"sensornode\", col_wrap=3, height=2, col_order=col_order, sharey=False)\n",
    "    g.map_dataframe(sns.barplot,  x=\"shap_value\", y=\"feature\", hue=\"fire_label\", orient ='h', palette=\"Set1\", errorbar=None)\n",
    "    g.add_legend()\n",
    "    g.set_axis_labels(\"Shap Value\", \"Feature\")\n",
    "    # Save the plot to a PNG file\n",
    "    file_name = f'shap_values_max_abs_mean_network_class_based_barplot_{scaler_name}.png'\n",
    "    # Create the file path using an f-string\n",
    "    file_path = os.path.join(directory_export, file_name)\n",
    "    plt.savefig(file_path)\n",
    "    \n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    df_max_shap_values_fire_sum = pd.concat(dfs_max_shap_values_fire, axis=1)\n",
    "    df_max_shap_values_fire_sum = df_max_shap_values_fire_sum.loc[:, ~df_max_shap_values_fire_sum.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7852bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting information from the 'feature' column\n",
    "df_max_shap_values_fire_sum['number'] = df_max_shap_values_fire_sum['feature'].str.extract(r'_(\\d+)$')\n",
    "df_max_shap_values_fire_sum['name'] = df_max_shap_values_fire_sum['feature'].str.split('_').str[0]\n",
    "\n",
    "# Creating the transformed dataframe\n",
    "df_max_shap_values_fire_sum_transformed = df_max_shap_values_fire_sum.pivot(index='number', columns='name', values='shap_value_fire_max')\n",
    "\n",
    "# Converting 'number' column to numeric for sorting\n",
    "df_max_shap_values_fire_sum_transformed.index = pd.to_numeric(df_max_shap_values_fire_sum_transformed.index)\n",
    "\n",
    "# Sorting the rows by the \"name number\"\n",
    "df_max_shap_values_fire_sum_transformed = df_max_shap_values_fire_sum_transformed.round(2).sort_index()\n",
    "\n",
    "# Displaying the transformed dataframe\n",
    "print(df_max_shap_values_fire_sum_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
