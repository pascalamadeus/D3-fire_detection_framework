{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33544866",
   "metadata": {},
   "source": [
    "# Detecting Early Fire Indicator Patterns in Multivariate-Time Series Based on a Multi-Sensor Node Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73ba11",
   "metadata": {},
   "source": [
    "## Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2514a4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "# Standard libaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()   \n",
    "import configparser\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import inspect\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import math\n",
    "\n",
    "# skLearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "#Sktime\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sktime.datatypes import check_raise\n",
    "from sktime.datatypes import mtype\n",
    "from sktime.datatypes import check_is_mtype\n",
    "from sktime.transformations.panel.padder import PaddingTransformer\n",
    "from sktime.transformations.series.summarize import SummaryTransformer\n",
    "from sktime.datatypes import convert_to\n",
    "from sktime.datatypes import convert\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "\n",
    "# Additional\n",
    "import matplotlib.dates as mdates\n",
    "import joblib\n",
    "import time # to claculate the runtime of models\n",
    "from pathlib import Path \n",
    "import pymannkendall as mk # Kendall tau trend package\n",
    "\n",
    "# Internal Packages\n",
    "from analyse_df import analyse_df\n",
    "from rename_columns import rename_columns\n",
    "import plot_settings\n",
    "\n",
    "# SHAP Explanation\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a6218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "data_path = os.path.join(current_dir, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c97ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_export = os.path.join(current_dir, 'export')\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_export):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(directory_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7e8f5",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c7536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "# X_t # multivariate time series including all sensor node positions\n",
    "# X_t_j # multivariate time series for one sensor node position j\n",
    "# X_t_j_m # univariate time series for one sensor node position and one measurement m\n",
    "\n",
    "# I_w # non-overlapping interval with interval length w\n",
    "w = 15 # interval length w\n",
    "l = 10 # length of one subsequence\n",
    "# S_l # non-overlappung subsequence within interval I with subsequence length l (schould be the smallest possible intrval, 10s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aee94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features\n",
    "FEATURES_STATIC = ['CO_Room',\n",
    "                    'H2_Room',\n",
    "                    'VOC_Room_RAW',\n",
    "                    'PM05_Room',\n",
    "                    'PM10_Room',\n",
    "                   ]\n",
    "\n",
    "FEATURES_TREND = ['CO_Room_Trend',\n",
    "                    'H2_Room_Trend',\n",
    "                    'VOC_Room_RAW_Trend',\n",
    "                    'PM05_Room_Trend',\n",
    "                    'PM10_Room_Trend', \n",
    "                    ]\n",
    "\n",
    "# Combine to one FEATURES list\n",
    "FEATURES = FEATURES_STATIC #+ FEATURES_TREND"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18a4ee",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6fee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "# Construct the absolute path to the data file\n",
    "file_name = 'elba_dataset_pp_3.csv'\n",
    "data_file_path = os.path.join(data_path, file_name)\n",
    "\n",
    "X_t = pd.read_csv(data_file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb377161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to int64 data type\n",
    "X_t['Date'] = X_t['Date'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca0511e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only number of sensornode instead of string name\n",
    "X_t['Sensor_ID'] = X_t['Sensor_ID'].str[-2:].astype(int) # anpassen auf Zwei Stellen!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a9b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non relevant labels\n",
    "X_t = X_t.drop(columns = ['scenario_label', \n",
    "                          'progress_label', \n",
    "                          'anomaly_label', \n",
    "                          'ternary_label', \n",
    "                          'Motion_Room', \n",
    "                          'Motion_Room_Trend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb667197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "X_t = X_t.rename(columns={\"Date\": \"timepoints\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927e8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_t in mtype=\"pd-multiindex\" format for sktime\n",
    "# doc.: https://github.com/sktime/sktime/blob/main/examples/AA_datatypes_and_datasets.ipynb\n",
    "\n",
    "X_t.set_index(['Interval_label','Sensor_ID', 'timepoints'], inplace=True)\n",
    "\n",
    "X_t = X_t.groupby(level=[0,1,2], sort=True).sum()\n",
    "\n",
    "X_t['timepoints'] = X_t.groupby(['Interval_label','Sensor_ID']).cumcount()\n",
    "X_t['timepoints'] = X_t['timepoints'].apply(lambda x: x*10)\n",
    "\n",
    "X_t = X_t.set_index('timepoints', append=True)\n",
    "X_t = X_t.droplevel(2)\n",
    "\n",
    "# Filter featrue columns based on FEATURES\n",
    "# data_columns = [col for col in X_t.columns if col != 'fire_label'] # Backup\n",
    "data_columns = FEATURES\n",
    "\n",
    "# Pivot the DataFrame to have Sensor_ID levels as columns\n",
    "X_t_pivot = X_t.pivot_table(index=['Interval_label', 'timepoints'], columns='Sensor_ID', values=data_columns)\n",
    "\n",
    "# Flatten the column MultiIndex\n",
    "X_t_pivot.columns = [f\"{col[0]}_{col[1]}\" for col in X_t_pivot.columns]\n",
    "\n",
    "# Pivot the original DataFrame to get the label_column based on Interval_label\n",
    "label_column_df = X_t[['fire_label']].reset_index().drop_duplicates(subset=['Interval_label'])\n",
    "label_column_df = label_column_df.set_index('Interval_label')\n",
    "label_column_df = label_column_df.drop(columns=['Sensor_ID', 'timepoints'])\n",
    "\n",
    "# Add label via list\n",
    "# Create list of indexes from df\n",
    "list_interval_indexes = list(X_t_pivot.index.get_level_values(0))\n",
    "\n",
    "# Create a dictionary from the DataFrame for quick lookup\n",
    "dict_indexes = label_column_df['fire_label'].to_dict()\n",
    "\n",
    "# Replace values in the list with corresponding values from the DataFrame\n",
    "fire_labels_list = [dict_indexes[idx] for idx in list_interval_indexes]\n",
    "\n",
    "# Add label list to df\n",
    "X_t_pivot['fire_label'] = fire_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07327cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of sensor IDs\n",
    "list_sensornodes = ['8',\n",
    "                     '9',\n",
    "                     '10',\n",
    "                     '11',\n",
    "                     '12',\n",
    "                     '13',\n",
    "                     '14',\n",
    "                     '15',\n",
    "                     '16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c93eedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model for every unique sensor node \n",
    "for sensornode in list_sensornodes:\n",
    "    \n",
    "    # Filter columns containing \"2\" after the last \"_\"\n",
    "    locked_columns = [col for col in X_t_pivot.columns if col.split(\"_\")[-1] == sensornode]+ [\"fire_label\"]\n",
    "\n",
    "    # Create a new DataFrame with only the locked columns\n",
    "    data_temp = X_t_pivot[locked_columns]\n",
    "    \n",
    "    # rename the columns (delete the sensor node number)\n",
    "    data_temp.columns = [col.rsplit('_', 1)[0] if '_' in col else col for col in data_temp.columns]\n",
    "\n",
    "    # Derive Train and hold-out (test) set\n",
    "    split_interval = 1300\n",
    "    df_train = data_temp.loc[data_temp.index.get_level_values('Interval_label') <= split_interval]\n",
    "    df_test = data_temp.loc[data_temp.index.get_level_values('Interval_label') > split_interval]\n",
    "    \n",
    "    # Derive X_train, X_test etc. \n",
    "    feature_columns = data_temp.columns.difference(['fire'])\n",
    "    # X_train\n",
    "    X_train = df_train[feature_columns]\n",
    "    # X_test\n",
    "    X_test = df_test[feature_columns]\n",
    "    # y_train\n",
    "    y_train = df_train['fire'].groupby('Interval_label').first().to_numpy()\n",
    "    # y_test\n",
    "    y_test = df_test['fire'].groupby('Interval_label').first().to_numpy()\n",
    "    \n",
    "    # Scaling: static for train and test set, not per subsequence\n",
    "\n",
    "    # Define Scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    # Separate data columns for scaling\n",
    "    data_cols = X_train.columns\n",
    "    # X_train_scaled\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_train_scaled[data_cols] = scaler.fit_transform(X_train_scaled[data_cols])\n",
    "    # # X_test_scaled\n",
    "    X_test_scaled = X_test.copy()\n",
    "    X_test_scaled[data_cols] = scaler.fit_transform(X_test_scaled[data_cols])\n",
    "    \n",
    "    # Build model\n",
    "    clf = RocketClassifier(num_kernels=500) \n",
    "    clf.fit(X_train, y_train) #X_train_scaled\n",
    "    # Save model\n",
    "    # Construct the absolute path to the data file\n",
    "    file_name = f'model_rocket_{sensornode}.pkl'\n",
    "    data_file_path = os.path.join(directory_export, file_name)\n",
    "    \n",
    "    with open(data_file_path, 'wb') as file:\n",
    "        pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b2bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
